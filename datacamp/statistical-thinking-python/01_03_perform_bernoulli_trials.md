<!doctype html><html lang="en"><head><link rel="icon" type="image/png" href="/favicon.ico"><link href="/static/css/main.2aa30486.css" rel="stylesheet"><title data-react-helmet="true">The np.random module and Bernoulli trials | Python</title><link data-react-helmet="true" href="data:image/png;base64,bW9kdWxlLmV4cG9ydHMgPSBfX3dlYnBhY2tfcHVibGljX3BhdGhfXyArICIvc3RhdGljL21lZGlhL2FwcGxlLWljb24uNGZhMTNiMGYucG5nIjs=" rel="apple-touch-icon"><link data-react-helmet="true" href="data:image/png;base64,bW9kdWxlLmV4cG9ydHMgPSBfX3dlYnBhY2tfcHVibGljX3BhdGhfXyArICIvc3RhdGljL21lZGlhL2FwcGxlLWljb24uNGZhMTNiMGYucG5nIjs=" rel="apple-touch-icon-precomposed"><meta data-react-helmet="true" charset="utf-8"><meta data-react-helmet="true" http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta data-react-helmet="true" name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta data-react-helmet="true" name="fragment" content="!"><meta data-react-helmet="true" name="keywords" content="R, Python, Data analysis, interactive, learning"><meta data-react-helmet="true" name="description" content="Here is an example of The np.random module and Bernoulli trials: You can think of a Bernoulli trial as a flip of a possibly biased coin."><meta data-react-helmet="true" name="twitter:card" content="summary"><meta data-react-helmet="true" name="twitter:site" content="@DataCamp"><meta data-react-helmet="true" name="twitter:title" content="The np.random module and Bernoulli trials | Python"><meta data-react-helmet="true" name="twitter:description" content="Here is an example of The np.random module and Bernoulli trials: You can think of a Bernoulli trial as a flip of a possibly biased coin."><meta data-react-helmet="true" name="twitter:creator" content="@DataCamp"><meta data-react-helmet="true" name="twitter:image:src" content="/public/assets/images/var/twitter_share.png"><meta data-react-helmet="true" name="twitter:domain" content="www.datacamp.com"><meta data-react-helmet="true" property="og:title" content="The np.random module and Bernoulli trials | Python"><meta data-react-helmet="true" property="og:image" content="/public/assets/images/var/linkedin_share.png"><meta data-react-helmet="true" name="google-signin-clientid" content="892114885437-01a7plbsu1b2vobuhvnckmmanhb58h3a.apps.googleusercontent.com"><meta data-react-helmet="true" name="google-signin-scope" content="email profile"><meta data-react-helmet="true" name="google-signin-cookiepolicy" content="single_host_origin"><script data-react-helmet="true" async="true" src="https://compliance.datacamp.com/base.js"></script><script data-react-helmet="true">
      var dataLayerContent = {
        gtm_version: 2,
      };
      if (typeof window['dataLayer'] === 'undefined') {
        window['dataLayer'] = [dataLayerContent];
      } else {
        window['dataLayer'].push(dataLayerContent);
      }
    </script></head><body><script>window.PRELOADED_STATE = "[&quot;~#iM&quot;,[&quot;preFetchedData&quot;,[&quot;^0&quot;,[&quot;course&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,[&quot;^ &quot;,&quot;id&quot;,1549,&quot;title&quot;,&quot;Statistical Thinking in Python (Part 1)&quot;,&quot;description&quot;,&quot;After all of the hard work of acquiring data and getting them into a form you can work with, you ultimately want to make clear, succinct conclusions from them. This crucial last step of a data analysis pipeline hinges on the principles of statistical inference. In this course, you will start building the foundation you need to think statistically, speak the language of your data, and understand what your data is telling you. The foundations of statistical thinking took decades to build, but can be grasped much faster today with the help of computers. With the power of Python-based tools, you will rapidly get up-to-speed and begin thinking statistically by the end of this course.&quot;,&quot;short_description&quot;,&quot;Build the foundation you need to think statistically and to speak the language of your data.&quot;,&quot;author_field&quot;,null,&quot;author_bio&quot;,null,&quot;author_image&quot;,&quot;placeholder.png&quot;,&quot;nb_of_subscriptions&quot;,93913,&quot;slug&quot;,&quot;statistical-thinking-in-python-part-1&quot;,&quot;image_url&quot;,&quot;https://assets.datacamp.com/production/course_1549/shields/thumb/shield_image_course_1549_20191010-1-13inj9n?1570728356&quot;,&quot;image_thumbnail_url&quot;,&quot;https://assets.datacamp.com/production/course_1549/shields/thumb_home/shield_image_course_1549_20191010-1-13inj9n?1570728356&quot;,&quot;last_updated_on&quot;,&quot;15/05/2020&quot;,&quot;link&quot;,&quot;https://www.datacamp.com/courses/statistical-thinking-in-python-part-1&quot;,&quot;should_cache&quot;,true,&quot;type&quot;,&quot;datacamp&quot;,&quot;difficulty_level&quot;,1,&quot;state&quot;,&quot;live&quot;,&quot;university&quot;,null,&quot;sharing_links&quot;,[&quot;^ &quot;,&quot;twitter&quot;,&quot;http://bit.ly/1eWTMJh&quot;,&quot;facebook&quot;,&quot;http://bit.ly/1iS42Do&quot;],&quot;marketing_video&quot;,&quot;statistical-thinking-in-python-part-1-marketing-video&quot;,&quot;programming_language&quot;,&quot;python&quot;,&quot;paid&quot;,true,&quot;time_needed&quot;,null,&quot;xp&quot;,4550,&quot;topic_id&quot;,7,&quot;technology_id&quot;,2,&quot;reduced_outline&quot;,null,&quot;runtime_config&quot;,null,&quot;lti_only&quot;,false,&quot;instructors&quot;,[[&quot;^ &quot;,&quot;id&quot;,353914,&quot;marketing_biography&quot;,&quot;Lecturer at the California Institute of Technology&quot;,&quot;biography&quot;,&quot;Justin Bois is a Teaching Professor in the Division of Biology and Biological Engineering at the California Institute of Technology. He teaches nine different classes there, nearly all of which heavily feature Python. He is dedicated to empowering students in the biological sciences with quantitative tools, particularly data analysis skills. Beyond biologists, he is thrilled to develop courses for DataCamp, whose students are an excited bunch of burgeoning data scientists!&quot;,&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/users/avatars/000/353/914/square/Screenshot_2016-10-07_18.54.27.png?1475880916&quot;,&quot;full_name&quot;,&quot;Justin Bois&quot;,&quot;instructor_path&quot;,&quot;/instructors/bois&quot;]],&quot;collaborators&quot;,[[&quot;^ &quot;,&quot;^Q&quot;,&quot;https://assets.datacamp.com/users/avatars/000/804/054/square/headshot.png?1490022370&quot;,&quot;^R&quot;,&quot;Yashas Roy&quot;],[&quot;^ &quot;,&quot;^Q&quot;,&quot;https://assets.datacamp.com/users/avatars/000/469/499/square/hugo_bowne-anderson.png?1470132470&quot;,&quot;^R&quot;,&quot;Hugo Bowne-Anderson&quot;],[&quot;^ &quot;,&quot;^Q&quot;,&quot;https://assets.datacamp.com/users/avatars/000/596/949/square/VL.png?1477655089&quot;,&quot;^R&quot;,&quot;Vincent Lan&quot;]],&quot;datasets&quot;,[[&quot;^ &quot;,&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/469/datasets/8fb59b9a99957c3b9b1c82b623aea54d8ccbcd9f/2008_all_states.csv&quot;,&quot;name&quot;,&quot;2008 election results (all states)&quot;],[&quot;^ &quot;,&quot;^V&quot;,&quot;https://assets.datacamp.com/production/repositories/469/datasets/e079fddb581197780e1a7b7af2aeeff7242535f0/2008_swing_states.csv&quot;,&quot;^W&quot;,&quot;2008 election results (swing states)&quot;],[&quot;^ &quot;,&quot;^V&quot;,&quot;https://assets.datacamp.com/production/repositories/469/datasets/7507bfed990379f246b4f166ea8a57ecf31c6c9d/belmont.csv&quot;,&quot;^W&quot;,&quot;Belmont Stakes&quot;],[&quot;^ &quot;,&quot;^V&quot;,&quot;https://assets.datacamp.com/production/repositories/469/datasets/df23780d215774ff90be0ea93e53f4fb5ebbade8/michelson_speed_of_light.csv&quot;,&quot;^W&quot;,&quot;Speed of light&quot;]],&quot;tracks&quot;,[[&quot;^ &quot;,&quot;path&quot;,&quot;/tracks/data-science-for-everyone&quot;,&quot;title_with_subtitle&quot;,&quot;Data Science for Everyone&quot;],[&quot;^ &quot;,&quot;^Y&quot;,&quot;/tracks/machine-learning-for-everyone&quot;,&quot;^Z&quot;,&quot;Machine Learning for Everyone&quot;],[&quot;^ &quot;,&quot;^Y&quot;,&quot;/tracks/data-scientist-with-python&quot;,&quot;^Z&quot;,&quot;Data Scientist  with Python&quot;],[&quot;^ &quot;,&quot;^Y&quot;,&quot;/tracks/statistics-fundamentals-with-python&quot;,&quot;^Z&quot;,&quot;Statistics Fundamentals  with Python&quot;]],&quot;prerequisites&quot;,[[&quot;^ &quot;,&quot;^Y&quot;,&quot;/courses/python-data-science-toolbox-part-2&quot;,&quot;^1&quot;,&quot;Python Data Science Toolbox (Part 2)&quot;]],&quot;time_needed_in_hours&quot;,3,&quot;seo_title&quot;,&quot;Statistical Thinking in Python (Part 1)&quot;,&quot;seo_description&quot;,&quot;Build the foundation you need to think statistically and to speak the language of your data.&quot;,&quot;archived_at&quot;,null,&quot;original_image_url&quot;,&quot;https://assets.datacamp.com/production/course_1549/shields/original/shield_image_course_1549_20191010-1-13inj9n?1570728356&quot;,&quot;external_slug&quot;,&quot;statistical-thinking-in-python-part-1&quot;,&quot;chapters&quot;,[[&quot;^ &quot;,&quot;id&quot;,3932,&quot;title_meta&quot;,null,&quot;^1&quot;,&quot;Graphical exploratory data analysis&quot;,&quot;^2&quot;,&quot;Before diving into sophisticated statistical inference techniques, you should first explore your data by plotting them and computing simple summary statistics. This process, called exploratory data analysis, is a crucial first step in statistical analysis of data.&quot;,&quot;number&quot;,1,&quot;^8&quot;,&quot;graphical-exploratory-data-analysis&quot;,&quot;nb_exercises&quot;,15,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;21/01/2020&quot;,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1549/slides/ch1_slides.pdf&quot;,&quot;free_preview&quot;,true,&quot;xp&quot;,1100,&quot;number_of_videos&quot;,5,&quot;exercises&quot;,[[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Introduction to Exploratory Data Analysis&quot;,&quot;aggregate_xp&quot;,50,&quot;^18&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=1&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;Tukey&#39;s comments on EDA&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=2&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;Advantages of graphical EDA&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=3&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Plotting a histogram&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=4&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Plotting a histogram of iris data&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=5&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Axis labels!&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=6&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Adjusting the number of bins in a histogram&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=7&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Plot all of your data: Bee swarm plots&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=8&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Bee swarm plot&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=9&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;MultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;Interpreting a bee swarm plot&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=10&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Plot all of your data: ECDFs&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=11&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Computing the ECDF&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=12&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Plotting the ECDF&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,13,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=13&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Comparison of ECDFs&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,14,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=14&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Onward toward the whole story!&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,15,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=15&quot;]]],[&quot;^ &quot;,&quot;id&quot;,3933,&quot;^17&quot;,null,&quot;^1&quot;,&quot;Quantitative exploratory data analysis&quot;,&quot;^2&quot;,&quot;In this chapter, you will compute useful summary statistics, which serve to concisely describe salient features of a dataset with a few numbers.&quot;,&quot;^18&quot;,2,&quot;^8&quot;,&quot;quantitative-exploratory-data-analysis&quot;,&quot;^19&quot;,15,&quot;^1:&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1;&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;21/01/2020&quot;,&quot;^1&lt;&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1549/slides/ch2_slides.pdf&quot;,&quot;^1=&quot;,null,&quot;xp&quot;,1200,&quot;^1&gt;&quot;,4,&quot;^1?&quot;,[[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Introduction to summary statistics: The sample mean and median&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=1&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;Means and medians&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=2&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Computing means&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=3&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Percentiles, outliers, and box plots&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=4&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Computing percentiles&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=5&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Comparing percentiles to ECDF&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=6&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Box-and-whisker plot&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=7&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Variance and standard deviation&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=8&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Computing the variance&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=9&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;The standard deviation and the variance&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=10&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Covariance and the Pearson correlation coefficient&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=11&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Scatter plots&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=12&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;MultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;Variance and covariance by looking&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,13,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=13&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Computing the covariance&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,14,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=14&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Computing the Pearson correlation coefficient&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,15,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=15&quot;]]],[&quot;^ &quot;,&quot;id&quot;,3934,&quot;^17&quot;,null,&quot;^1&quot;,&quot;Thinking probabilistically-- Discrete variables&quot;,&quot;^2&quot;,&quot;Statistical inference rests upon probability. Because we can very rarely say anything meaningful with absolute certainty from data, we use probabilistic language to make quantitative statements about data. In this chapter, you will learn how to think probabilistically about discrete quantities: those that can only take certain values, like integers.&quot;,&quot;^18&quot;,3,&quot;^8&quot;,&quot;thinking-probabilistically-discrete-variables&quot;,&quot;^19&quot;,15,&quot;^1:&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1;&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;21/01/2020&quot;,&quot;^1&lt;&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1549/slides/ch3_slides.pdf&quot;,&quot;^1=&quot;,null,&quot;xp&quot;,1150,&quot;^1&gt;&quot;,4,&quot;^1?&quot;,[[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Probabilistic logic and statistical inference&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=1&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;What is the goal of statistical inference?&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=2&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;Why do we use the language of probability?&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=3&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Random number generators and hacker statistics&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=4&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Generating random numbers using the np.random module&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=5&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;The np.random module and Bernoulli trials&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=6&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;How many defaults might we expect?&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=7&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Will the bank fail?&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=8&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Probability distributions and stories: The Binomial distribution&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=9&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Sampling out of the Binomial distribution&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=10&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Plotting the Binomial PMF&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=11&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Poisson processes and the Poisson distribution&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=12&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Relationship between Binomial and Poisson distributions&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,13,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=13&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;MultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;How many no-hitters in a season?&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,14,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=14&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Was 2015 anomalous?&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,15,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=15&quot;]]],[&quot;^ &quot;,&quot;id&quot;,3935,&quot;^17&quot;,null,&quot;^1&quot;,&quot;Thinking probabilistically-- Continuous variables&quot;,&quot;^2&quot;,&quot;Itâ€™s time to move onto continuous variables, such as those that can take on any fractional value. Many of the principles are the same, but there are some subtleties. At the end of this final chapter, you will be speaking the probabilistic language you need to launch into the inference techniques covered in the sequel to this course.&quot;,&quot;^18&quot;,4,&quot;^8&quot;,&quot;thinking-probabilistically-continuous-variables&quot;,&quot;^19&quot;,16,&quot;^1:&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1;&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;21/01/2020&quot;,&quot;^1&lt;&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1549/slides/ch4_slides.pdf&quot;,&quot;^1=&quot;,null,&quot;xp&quot;,1100,&quot;^1&gt;&quot;,5,&quot;^1?&quot;,[[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Probability density functions&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=1&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;MultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;Interpreting PDFs&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=2&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;MultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;Interpreting CDFs&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=3&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Introduction to the Normal distribution&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=4&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;The Normal PDF&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=5&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;The Normal CDF&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=6&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;The Normal distribution: Properties and warnings&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=7&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;MultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;Gauss and the 10 Deutschmark banknote&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=8&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Are the Belmont Stakes results Normally distributed?&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=9&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;What are the chances of a horse matching or beating Secretariat&#39;s record?&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=10&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;The Exponential distribution&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=11&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;Matching a story and a distribution&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=12&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;Waiting for the next Secretariat&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,13,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=13&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;If you have a story, you can simulate it!&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,14,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=14&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Distribution of no-hitters and cycles&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,15,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=15&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Final thoughts&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,16,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=16&quot;]]]]]]],&quot;chapter&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,[&quot;^ &quot;,&quot;id&quot;,3934,&quot;^17&quot;,null,&quot;^1&quot;,&quot;Thinking probabilistically-- Discrete variables&quot;,&quot;^2&quot;,&quot;Statistical inference rests upon probability. Because we can very rarely say anything meaningful with absolute certainty from data, we use probabilistic language to make quantitative statements about data. In this chapter, you will learn how to think probabilistically about discrete quantities: those that can only take certain values, like integers.&quot;,&quot;^18&quot;,3,&quot;^8&quot;,&quot;thinking-probabilistically-discrete-variables&quot;,&quot;^19&quot;,15,&quot;^1:&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1;&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^;&quot;,&quot;21/01/2020&quot;,&quot;^1&lt;&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1549/slides/ch3_slides.pdf&quot;,&quot;^1=&quot;,null,&quot;xp&quot;,1150,&quot;^1&gt;&quot;,4,&quot;^1?&quot;,[[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Probabilistic logic and statistical inference&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=1&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;What is the goal of statistical inference?&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=2&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;Why do we use the language of probability?&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=3&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Random number generators and hacker statistics&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=4&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Generating random numbers using the np.random module&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=5&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;The np.random module and Bernoulli trials&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=6&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;How many defaults might we expect?&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=7&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Will the bank fail?&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=8&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Probability distributions and stories: The Binomial distribution&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=9&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Sampling out of the Binomial distribution&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=10&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Plotting the Binomial PMF&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=11&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1&quot;,&quot;Poisson processes and the Poisson distribution&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=12&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Relationship between Binomial and Poisson distributions&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,13,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=13&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;MultipleChoiceExercise&quot;,&quot;^1&quot;,&quot;How many no-hitters in a season?&quot;,&quot;^1@&quot;,50,&quot;^18&quot;,14,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=14&quot;],[&quot;^ &quot;,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1&quot;,&quot;Was 2015 anomalous?&quot;,&quot;^1@&quot;,100,&quot;^18&quot;,15,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=15&quot;]]]]],&quot;exercises&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,[[&quot;^ &quot;,&quot;id&quot;,39983,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;assignment&quot;,null,&quot;^1&quot;,&quot;Probabilistic logic and statistical inference&quot;,&quot;sample_code&quot;,&quot;&quot;,&quot;instructions&quot;,null,&quot;^18&quot;,1,&quot;sct&quot;,&quot;&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;attachments&quot;,null,&quot;xp&quot;,50,&quot;possible_answers&quot;,[],&quot;feedbacks&quot;,[],&quot;question&quot;,&quot;&quot;,&quot;video_link&quot;,null,&quot;video_hls&quot;,null,&quot;aspect_ratio&quot;,56.25,&quot;projector_key&quot;,&quot;course_1549_dae09d85f518dfd3e46d3c0427726dde&quot;,&quot;language&quot;,&quot;python&quot;,&quot;randomNumber&quot;,0.4574118107856051,&quot;externalId&quot;,39983],[&quot;^ &quot;,&quot;id&quot;,39989,&quot;^&gt;&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;Why do we do statistical inference?&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;What is the goal of statistical inference?&quot;,&quot;^1B&quot;,&quot;&quot;,&quot;^1C&quot;,null,&quot;^18&quot;,2,&quot;sct&quot;,&quot;&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;&quot;,&quot;^1F&quot;,&quot;&lt;p&gt;We do statistical inference to draw conclusions about our data: These may be probabilistic, actionable, or general conclusions, but the goal is to be able to make decisions based on these conclusions.&lt;/p&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,50,&quot;^1H&quot;,[&quot;To draw probabilistic conclusions about what we might expect if we collected the same data again.&quot;,&quot;To draw actionable conclusions from data.&quot;,&quot;To draw more general conclusions from relatively few data or observations.&quot;,&quot;[All of these.]&quot;],&quot;^1I&quot;,[&quot;Yes, this is true, but there is more!&quot;,&quot;Yes, this is true, but there is more!&quot;,&quot;Yes, this is true, but there is more!&quot;,&quot;Correct! Statistical inference involves taking your data to probabilistic conclusions about what you would expect if you took even more data, and you can make decisions based on these conclusions.&quot;],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,0.5578316998821693,&quot;^1Q&quot;,39989],[&quot;^ &quot;,&quot;id&quot;,45888,&quot;^&gt;&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;Which of the following is &lt;em&gt;not&lt;/em&gt; a reason why we use probabilistic language in statistical inference?&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Why do we use the language of probability?&quot;,&quot;^1B&quot;,&quot;&quot;,&quot;^1C&quot;,null,&quot;^18&quot;,3,&quot;sct&quot;,&quot;&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;&quot;,&quot;^1F&quot;,&quot;&lt;p&gt;While the English word &#39;probably&#39; is indeed not very precise, in the mathematical context, probabilistic language is actually very precise and provides us a measure of uncertainty.&lt;/p&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,50,&quot;^1H&quot;,[&quot;Probability provides a measure of uncertainty.&quot;,&quot;[Probabilistic language is not very precise.]&quot;,&quot;Data are almost never exactly the same when acquired again, and probability allows us to say how much we expect them to vary.&quot;],&quot;^1I&quot;,[&quot;Probability is a measure of uncertainty, and this is crucial because we can quantify what we might expect if the data were acquired again.&quot;,&quot;Correct. Probabilistic language is in fact very precise. It precisely describes uncertainty.&quot;,&quot;Incorrect. We need probability to say how data might vary if acquired again.&quot;],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,0.3147939331894314,&quot;^1Q&quot;,45888],[&quot;^ &quot;,&quot;id&quot;,40351,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1A&quot;,null,&quot;^1&quot;,&quot;Random number generators and hacker statistics&quot;,&quot;^1B&quot;,&quot;&quot;,&quot;^1C&quot;,null,&quot;^18&quot;,4,&quot;sct&quot;,&quot;&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;xp&quot;,50,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1K&quot;,null,&quot;^1L&quot;,null,&quot;^1M&quot;,56.25,&quot;^1N&quot;,&quot;course_1549_05960404f6ed0bdb74b810aeaac8318d&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,0.9728646761118771,&quot;^1Q&quot;,40351],[&quot;^ &quot;,&quot;id&quot;,39984,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;We will be hammering the &lt;code&gt;np.random&lt;/code&gt; module for the rest of this course and its sequel. Actually, you will probably call functions from this module more than any other while wearing your hacker statistician hat. Let&#39;s start by taking its simplest function, &lt;code&gt;np.random.random()&lt;/code&gt; for a test spin. The function returns a random number between zero and one. Call &lt;code&gt;np.random.random()&lt;/code&gt; a few times in the IPython shell. You should see numbers jumping around between zero and one.&lt;/p&gt;\\n&lt;p&gt;In this exercise, we&#39;ll generate lots of random numbers between zero and one, and then plot a histogram of the results. If the numbers are truly random, all bars in the histogram should be of (close to) equal height.&lt;/p&gt;\\n&lt;p&gt;You may have noticed that, in the video, Justin generated 4 random numbers by passing the keyword argument &lt;code&gt;size=4&lt;/code&gt; to &lt;code&gt;np.random.random()&lt;/code&gt;. Such an approach is more efficient than a &lt;code&gt;for&lt;/code&gt; loop: in this exercise, however, you will write a &lt;code&gt;for&lt;/code&gt; loop to experience hacker statistics as the practice of repeating an experiment over and over again.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Generating random numbers using the np.random module&quot;,&quot;^1B&quot;,&quot;# Seed the random number generator\\n\\n\\n# Initialize random numbers: random_numbers\\nrandom_numbers = ____\\n\\n# Generate random numbers by looping over range(100000)\\nfor i in ____:\\n    random_numbers[i] = ____\\n\\n# Plot a histogram\\n_ = plt.hist(____)\\n\\n# Show the plot\\nplt.show()\\n&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Seed the random number generator using the seed &lt;code&gt;42&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Initialize an empty array, &lt;code&gt;random_numbers&lt;/code&gt;, of 100,000 entries to store the random numbers. Make sure you use &lt;code&gt;np.empty(100000)&lt;/code&gt; to do this.&lt;/li&gt;\\n&lt;li&gt;Write a &lt;code&gt;for&lt;/code&gt; loop to draw 100,000 random numbers using &lt;code&gt;np.random.random()&lt;/code&gt;, storing them in the &lt;code&gt;random_numbers&lt;/code&gt; array. To do so, loop over &lt;code&gt;range(100000)&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Plot a histogram of &lt;code&gt;random_numbers&lt;/code&gt;. It is not necessary to label the axes in this case because we are just checking the random number generator. Hit &#39;Submit Answer&#39; to show your plot.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,5,&quot;sct&quot;,&quot;Ex().check_function(&#39;numpy.random.seed&#39;).check_args(0).has_equal_value()\\n\\nEx().check_function(&#39;numpy.empty&#39;, signature = False).check_args(0).has_equal_value()\\n\\nEx().check_correct(\\n  check_object(&#39;random_numbers&#39;).has_equal_value(),\\n  check_for_loop().multi(\\n    check_iter().has_equal_value(),\\n    multi(\\n      check_body().check_function(&#39;numpy.random.random&#39;, signature = False),\\n      check_body().has_equal_ast(incorrect_msg = &#39;Check the first for loop. Did you correctly specify the body?&#39;)\\n    )\\n  )\\n)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.hist\\&quot;).check_args(0).has_equal_value()\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\nsuccess_msg(\\&quot;Good work! The histogram is almost exactly flat across the top, indicating that there is equal chance that a randomly-generated number is in any of the bins of the histogram.\\&quot;)&quot;,&quot;^1D&quot;,&quot;import numpy as np\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()&quot;,&quot;^1E&quot;,&quot;# Seed the random number generator\\nnp.random.seed(42)\\n\\n# Initialize random numbers: random_numbers\\nrandom_numbers = np.empty(100000)\\n\\n# Generate random numbers by looping over range(100000)\\nfor i in range(100000):\\n    random_numbers[i] = np.random.random()\\n\\n# Plot a histogram\\n_ = plt.hist(random_numbers)\\n\\n# Show the plot\\nplt.show()\\n&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;You can set a random seed using the function &lt;code&gt;np.random.seed()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;To initialize an empty array of, say, 5 entries, you can use &lt;code&gt;np.empty(5)&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;&lt;code&gt;for i in range(100000)&lt;/code&gt; initializes a for loop that will range over 100,000 values; for each of the 100,000 entries, you can use &lt;code&gt;np.random.random()&lt;/code&gt; to draw a random number. Be sure to store the result in the appropriate index &lt;code&gt;i&lt;/code&gt; of &lt;code&gt;random_numbers&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;You can pass in &lt;code&gt;random_numbers&lt;/code&gt; to &lt;code&gt;plt.hist&lt;/code&gt; to plot a histogram.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,0.45736793283436694,&quot;^1Q&quot;,39984],[&quot;^ &quot;,&quot;id&quot;,47490,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;You can think of a Bernoulli trial as a flip of a possibly biased coin. Specifically, each coin flip has a probability \\\\(p\\\\) of landing heads (success) and probability \\\\(1-p\\\\) of landing tails (failure). In this exercise, you will write a function to perform &lt;code&gt;n&lt;/code&gt; Bernoulli trials, &lt;code&gt;perform_bernoulli_trials(n, p)&lt;/code&gt;, which returns the number of successes out of &lt;code&gt;n&lt;/code&gt; Bernoulli trials, each of which has probability &lt;code&gt;p&lt;/code&gt; of success. To perform each Bernoulli trial, use the &lt;code&gt;np.random.random()&lt;/code&gt; function, which returns a random number between zero and one.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;The np.random module and Bernoulli trials&quot;,&quot;^1B&quot;,&quot;def perform_bernoulli_trials(n, p):\\n    \\&quot;\\&quot;\\&quot;Perform n Bernoulli trials with success probability p\\n    and return number of successes.\\&quot;\\&quot;\\&quot;\\n    # Initialize number of successes: n_success\\n    n_success = ____\\n\\n    # Perform trials\\n    for i in ____:\\n        # Choose random number between zero and one: random_number\\n\\n\\n        # If less than p, it&#39;s a success so add one to n_success\\n        if ____:\\n            ____\\n\\n    return n_success&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Define a function with signature &lt;code&gt;perform_bernoulli_trials(n, p)&lt;/code&gt;.&lt;ul&gt;\\n&lt;li&gt;Initialize to zero a variable &lt;code&gt;n_success&lt;/code&gt; the counter of &lt;code&gt;True&lt;/code&gt;s, which are Bernoulli trial successes.&lt;/li&gt;\\n&lt;li&gt;Write a &lt;code&gt;for&lt;/code&gt; loop where you perform a Bernoulli trial in each iteration  and increment the number of success if the result is &lt;code&gt;True&lt;/code&gt;. Perform &lt;code&gt;n&lt;/code&gt; iterations by looping over &lt;code&gt;range(n)&lt;/code&gt;.&lt;ul&gt;\\n&lt;li&gt;To perform a Bernoulli trial, choose a random number between zero and one using &lt;code&gt;np.random.random()&lt;/code&gt;. If the number you chose is less than &lt;code&gt;p&lt;/code&gt;, increment &lt;code&gt;n_success&lt;/code&gt; (use the &lt;code&gt;+= 1&lt;/code&gt; operator to achieve this).&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;\\n&lt;li&gt;The function returns the number of successes &lt;code&gt;n_success&lt;/code&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,6,&quot;sct&quot;,&quot;Ex().check_function_def(\\&quot;perform_bernoulli_trials\\&quot;).check_correct(\\n  multi(\\n    check_call(\\&quot;f(10, 0.3)\\&quot;).has_equal_value(pre_code=\\&quot;np.random.seed(42)\\&quot;),\\n    check_call(\\&quot;f(10, 0.9)\\&quot;).has_equal_value(pre_code=\\&quot;np.random.seed(42)\\&quot;)    \\n  ),  \\n  multi(\\n    check_args(0).has_equal_ast(),\\n    check_args(1).has_equal_ast(),\\n    check_body().multi(\\n      has_equal_ast(code = \\&quot;n_success = 0\\&quot;, incorrect_msg = \\&quot;Did you correctly intialize the number of successes?\\&quot;),\\n      check_for_loop().multi(\\n        check_iter().has_equal_ast(),\\n        check_body().multi(\\n          check_function(\\&quot;numpy.random.random\\&quot;, signature = sig_from_obj(\\&quot;numpy.random\\&quot;)),\\n          check_if_else().multi(\\n            check_test().has_equal_ast(incorrect_msg = \\&quot;Make sure to test if `random_number &lt; p`.\\&quot;),\\n            check_body().check_or(\\n              has_equal_ast(incorrect_msg = \\&quot;Did you increment `n_success` by `1`?\\&quot;),\\n              has_equal_ast(code = \\&quot;n_success = n_success + 1\\&quot;, incorrect_msg = \\&quot;Did you increment `n_success` by `1`?\\&quot;),\\n              has_equal_ast(code = \\&quot;n_success = 1 + n_success\\&quot;, incorrect_msg = \\&quot;Did you increment `n_success` by `1`?\\&quot;)\\n            )\\n          )\\n        )\\n      )\\n    )\\n  )\\n)\\n\\nsuccess_msg(\\&quot;Good work!\\&quot;)&quot;,&quot;^1D&quot;,&quot;import numpy as np\\nnp.random.seed(42)&quot;,&quot;^1E&quot;,&quot;def perform_bernoulli_trials(n, p):\\n    \\&quot;\\&quot;\\&quot;Perform n Bernoulli trials with success probability p\\n    and return number of successes.\\&quot;\\&quot;\\&quot;\\n    # Initialize number of successes: n_success\\n    n_success = 0\\n\\n    # Perform trials\\n    for i in range(n):\\n        # Choose random number between zero and one: random_number\\n        random_number = np.random.random()\\n\\n        # If less than p, it&#39;s a success  so add one to n_success\\n        if random_number &lt; p:\\n            n_success += 1\\n\\n    return n_success&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;The number of &#39;successes&#39;, &lt;code&gt;n_success&lt;/code&gt;, we start with is 0, so you need to initialize &lt;code&gt;n_success&lt;/code&gt; before we can perform our trials.&lt;/li&gt;\\n&lt;li&gt;For each of the &lt;code&gt;n&lt;/code&gt; trials, use &lt;code&gt;np.random.random()&lt;/code&gt; to choose a random number between 0 and 1 and store the result in &lt;code&gt;random_number&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;If the &lt;code&gt;random_number&lt;/code&gt; selected is &lt;code&gt;&amp;lt;&lt;/code&gt; than the probability of success &lt;code&gt;p&lt;/code&gt;, the trial is considered a success and &lt;code&gt;n_success&lt;/code&gt; will need to be incremented with &lt;code&gt;+=&lt;/code&gt;. Be sure to pay attention to indentation of your &lt;code&gt;for&lt;/code&gt; loop and &lt;code&gt;if&lt;/code&gt; statement.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,0.6138412522323577,&quot;^1Q&quot;,47490],[&quot;^ &quot;,&quot;id&quot;,39985,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;Let&#39;s say a bank made 100 mortgage loans. It is possible that anywhere between 0 and 100 of the loans will be defaulted upon. You would like to know the probability of getting a given number of defaults, given that the probability of a default is &lt;code&gt;p = 0.05&lt;/code&gt;. To investigate this, you will do a simulation. You will perform 100 Bernoulli trials using the &lt;code&gt;perform_bernoulli_trials()&lt;/code&gt; function you wrote in the previous exercise and record how many defaults we get. Here, a success is a default. (Remember that the word \\&quot;success\\&quot; just means that the Bernoulli trial evaluates to &lt;code&gt;True&lt;/code&gt;, i.e., did the loan recipient default?) You will do this for another 100 Bernoulli trials. And again and again until we have tried it 1000 times. Then, you will plot a histogram describing the probability of the number of defaults.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;How many defaults might we expect?&quot;,&quot;^1B&quot;,&quot;# Seed random number generator\\n\\n\\n# Initialize the number of defaults: n_defaults\\n\\n\\n# Compute the number of defaults\\nfor i in ____:\\n    n_defaults[i] = ____\\n\\n\\n# Plot the histogram with default number of bins; label your axes\\n_ = plt.hist(____, ____)\\n_ = plt.xlabel(&#39;number of defaults out of 100 loans&#39;)\\n_ = plt.ylabel(&#39;probability&#39;)\\n\\n# Show the plot\\n&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Seed the random number generator to 42.&lt;/li&gt;\\n&lt;li&gt;Initialize &lt;code&gt;n_defaults&lt;/code&gt;, an empty array, using &lt;code&gt;np.empty()&lt;/code&gt;. It should contain 1000 entries, since we are doing 1000 simulations.&lt;/li&gt;\\n&lt;li&gt;Write a &lt;code&gt;for&lt;/code&gt; loop with &lt;code&gt;1000&lt;/code&gt; iterations  to compute the number of defaults per 100 loans using the &lt;code&gt;perform_bernoulli_trials()&lt;/code&gt; function. It accepts two arguments: the number of trials &lt;code&gt;n&lt;/code&gt; - in this case 100 - and the probability of success &lt;code&gt;p&lt;/code&gt; - in this case the probability of a default, which is &lt;code&gt;0.05&lt;/code&gt;. On each iteration of the loop store the result in an entry of &lt;code&gt;n_defaults&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Plot a histogram of &lt;code&gt;n_defaults&lt;/code&gt;. Include the &lt;code&gt;normed=True&lt;/code&gt; keyword argument so that the height of the bars of the histogram indicate the probability.&lt;/li&gt;\\n&lt;li&gt;Show your plot.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,7,&quot;sct&quot;,&quot;Ex().check_function(\\&quot;numpy.random.seed\\&quot;).check_args(0).has_equal_value()\\nEx().check_correct(\\n  check_object(\\&quot;n_defaults\\&quot;).has_equal_value(),\\n  check_function(\\&quot;numpy.empty\\&quot;, signature = sig_from_params(param(\\&quot;shape\\&quot;, param.POSITIONAL_OR_KEYWORD))).check_args(0).has_equal_value()\\n)\\nEx().check_for_loop().multi(\\n\\tcheck_iter().check_function(\\&quot;range\\&quot;, signature = sig_from_params(param(\\&quot;stop\\&quot;, param.POSITIONAL_ONLY))).check_args(0).has_equal_value(),\\n  \\tcheck_body().check_function(\\&quot;perform_bernoulli_trials\\&quot;).multi(\\n    \\tcheck_args(0).has_equal_value(),\\n      \\tcheck_args(1).has_equal_value()\\n    )\\n)\\nEx().check_function(\\&quot;matplotlib.pyplot.hist\\&quot;).multi(\\n\\tcheck_args(0).has_equal_ast(),\\n  \\tcheck_args(\\&quot;normed\\&quot;).has_equal_value()\\n)\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\nsuccess_msg(\\&quot;Nice work! This is actually not an optimal way to plot a histogram when the results are known to be integers. We will revisit this in forthcoming exercises.\\&quot;)&quot;,&quot;^1D&quot;,&quot;import numpy as np\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()\\n\\ndef bernoulli_trial(p):\\n    return np.random.random() &lt; p\\n\\ndef perform_bernoulli_trials(n, p):\\n    n_success = 0\\n    for _ in range(n):\\n        n_success += bernoulli_trial(p)\\n    return n_success&quot;,&quot;^1E&quot;,&quot;# Seed random number generator\\nnp.random.seed(42)\\n\\n# Initialize the number of defaults: n_defaults\\nn_defaults = np.empty(1000)\\n\\n# Compute the number of defaults\\nfor i in range(1000):\\n    n_defaults[i] = perform_bernoulli_trials(100, 0.05)\\n\\n# Plot the histogram with default number of bins; label your axes\\n_ = plt.hist(n_defaults, normed=True)\\n_ = plt.xlabel(&#39;number of defaults out of 100 loans&#39;)\\n_ = plt.ylabel(&#39;probability&#39;)\\n\\n# Show the plot\\nplt.show()&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;As you did earlier, you can set the random seed using &lt;code&gt;np.random.seed(42)&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;An empty array can be initialized using &lt;code&gt;np.empty()&lt;/code&gt;, with the number of entries specified inside the parentheses.&lt;/li&gt;\\n&lt;li&gt;Earlier, you looped over a range of 100,000 values and then n values using &lt;code&gt;for i in range(100000)&lt;/code&gt; and &lt;code&gt;for i in range(n)&lt;/code&gt;, respectively. Here, we need to loop over the number of entries we specified above. While we have 1000 entries, or simulations, we will only be performing 100 bernoulli trials, each of which has a probability of success &lt;code&gt;p = 0.05&lt;/code&gt;; these are the arguments that will need to be passed in to the &lt;code&gt;perform_bernoulli_trials()&lt;/code&gt; function that was defined in the previous exercise. For each trial, the result should be assigned to the appropriate index of &lt;code&gt;n_defaults&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;You can plot a histogram using &lt;code&gt;plt.hist()&lt;/code&gt;, but remember to specify &lt;code&gt;normed=True&lt;/code&gt; in addition to &lt;code&gt;n_defaults&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;plt.show()&lt;/code&gt; to display the plot.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,0.0663850191800277,&quot;^1Q&quot;,39985],[&quot;^ &quot;,&quot;id&quot;,39986,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;Plot the number of defaults you got from the previous exercise, in your namespace as &lt;code&gt;n_defaults&lt;/code&gt;, as a CDF. The &lt;code&gt;ecdf()&lt;/code&gt; function you wrote in the first chapter is available.&lt;/p&gt;\\n&lt;p&gt;If interest rates are such that the bank will lose money if 10 or more of its loans are defaulted upon, what is the probability that the bank will lose money?&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Will the bank fail?&quot;,&quot;^1B&quot;,&quot;# Compute ECDF: x, y\\n\\n\\n# Plot the ECDF with labeled axes\\n\\n\\n\\n\\n# Show the plot\\n\\n\\n# Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money\\n\\n\\n# Compute and print probability of losing money\\nprint(&#39;Probability of losing money =&#39;, n_lose_money / len(n_defaults))\\n&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Compute the &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; values for the ECDF of &lt;code&gt;n_defaults&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Plot the ECDF, making sure to label the axes. Remember to include &lt;code&gt;marker = &#39;.&#39;&lt;/code&gt; and &lt;code&gt;linestyle = &#39;none&#39;&lt;/code&gt; in addition to &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; in your call &lt;code&gt;plt.plot()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Show the plot.&lt;/li&gt;\\n&lt;li&gt;Compute the total number of entries in your &lt;code&gt;n_defaults&lt;/code&gt; array that were greater than or equal to 10. To do so, compute a boolean array that tells you whether a given entry of &lt;code&gt;n_defaults&lt;/code&gt; is &lt;code&gt;&amp;gt;= 10&lt;/code&gt;. Then sum all the entries in this array using &lt;code&gt;np.sum()&lt;/code&gt;. For example, &lt;code&gt;np.sum(n_defaults &amp;lt;= 5)&lt;/code&gt; would compute the number of defaults with 5 or &lt;em&gt;fewer&lt;/em&gt; defaults.&lt;/li&gt;\\n&lt;li&gt;The probability that the bank loses money is the fraction of &lt;code&gt;n_defaults&lt;/code&gt; that are greater than or equal to 10. Print this result by hitting &#39;Submit Answer&#39;!&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,8,&quot;sct&quot;,&quot;Ex().check_correct(\\n  multi(\\n    check_object(&#39;x&#39;).has_equal_value(),\\n    check_object(&#39;y&#39;).has_equal_value()\\n  ),\\n  check_function(&#39;ecdf&#39;).check_args(0).has_equal_value()\\n)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.plot\\&quot;, index = 0).multi(\\n  check_args([&#39;args&#39;, 0]).has_equal_value(),\\n  check_args([&#39;args&#39;, 1]).has_equal_value(),\\n  check_args([&#39;kwargs&#39;, &#39;marker&#39;]).has_equal_value(),\\n  check_args([&#39;kwargs&#39;, &#39;linestyle&#39;]).has_equal_value()\\n)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.xlabel\\&quot;).check_args(0)\\nEx().check_function(\\&quot;matplotlib.pyplot.ylabel\\&quot;).check_args(0)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\nEx().check_correct(\\n  check_object(&#39;n_lose_money&#39;).has_equal_value(),\\n  check_function(&#39;numpy.sum&#39;).check_args(0).has_equal_ast()\\n)\\n\\nEx().has_printout(0)\\n\\nsuccess_msg(\\&quot;As we might expect, we most likely get 5/100 defaults. But we still have about a 2% chance of getting 10 or more defaults out of 100 loans.\\&quot;)&quot;,&quot;^1D&quot;,&quot;import numpy as np\\nimport pandas as pd\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()\\n\\n# Load data from last time for speed\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1549/datasets/n_defaults.csv&#39;, header=None)\\nn_defaults = df[0].values\\n\\ndef bernoulli_trial(p):\\n    \\&quot;\\&quot;\\&quot;Perform Bernoulli trial with success probability p.\\&quot;\\&quot;\\&quot;\\n    # Choose random number between zero and one\\n    random_number = np.random.random()\\n\\n    # Check to see if it is less than p and return True, else return False\\n    if random_number &lt; p:\\n        return True\\n    return False\\n\\ndef ecdf(data):\\n    return np.sort(data), np.arange(1, len(data)+1) / len(data)&quot;,&quot;^1E&quot;,&quot;# Compute ECDF: x, y\\nx, y = ecdf(n_defaults)\\n\\n# Plot the CDF with labeled axes\\n_ = plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;)\\n_ = plt.xlabel(&#39;number of defaults out of 100&#39;)\\n_ = plt.ylabel(&#39;CDF&#39;)\\n\\n# Show the plot\\nplt.show()\\n\\n# Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money\\nn_lose_money = np.sum(n_defaults &gt;= 10)\\n\\n# Compute and print probability of losing money\\nprint(&#39;Probability of losing money =&#39;, n_lose_money / len(n_defaults))\\n&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;You can compute the &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; values for the ECDF by unpacking the output of &lt;code&gt;ecdf(n_defaults)&lt;/code&gt; appropriately.&lt;/li&gt;\\n&lt;li&gt;To plot the &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; values you just computed, pass them into the &lt;code&gt;plt.plot()&lt;/code&gt; function. Remember to include &lt;code&gt;marker = &#39;.&#39;&lt;/code&gt; and &lt;code&gt;linestyle = &#39;none&#39;&lt;/code&gt; as arguments inside &lt;code&gt;&#39;plt.plot()&lt;/code&gt;, as well as to give labels using &lt;code&gt;plt.xlabel()&lt;/code&gt; and &lt;code&gt;plt.ylabel()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;plt.show()&lt;/code&gt; to display the ECDF you just plotted.&lt;/li&gt;\\n&lt;li&gt;&lt;code&gt;np.sum()&lt;/code&gt; is a useful function when you need to compute &lt;em&gt;totals&lt;/em&gt; of any kind. You can take advantage of NumPy&#39;s vectorized operations by subsetting &lt;code&gt;n_defaults&lt;/code&gt; appropriately with &lt;code&gt;&amp;gt;=&lt;/code&gt; and passing it into &lt;code&gt;np.sum()&lt;/code&gt;. Be sure to assign the result to &lt;code&gt;n_lose_money&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,0.18777558662769356,&quot;^1Q&quot;,39986],[&quot;^ &quot;,&quot;id&quot;,39987,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1A&quot;,null,&quot;^1&quot;,&quot;Probability distributions and stories: The Binomial distribution&quot;,&quot;^1B&quot;,&quot;&quot;,&quot;^1C&quot;,null,&quot;^18&quot;,9,&quot;sct&quot;,&quot;&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;xp&quot;,50,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1K&quot;,null,&quot;^1L&quot;,null,&quot;^1M&quot;,56.25,&quot;^1N&quot;,&quot;course_1549_9b900d9ba13fadf0da9cd66248cc0840&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,0.25456993294015073,&quot;^1Q&quot;,39987],[&quot;^ &quot;,&quot;id&quot;,39988,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;Compute the probability mass function for the number of defaults we would expect for 100 loans as in the last section, but instead of simulating all of the Bernoulli trials, perform the sampling using &lt;code&gt;np.random.binomial()&lt;/code&gt;. This is identical to the calculation you did in the last set of exercises using your custom-written &lt;code&gt;perform_bernoulli_trials()&lt;/code&gt; function, but far more computationally efficient. Given this extra efficiency, we will take 10,000 samples instead of 1000. After taking the samples, plot the CDF as last time. This CDF that you are plotting is that of the Binomial distribution.&lt;/p&gt;\\n&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: For this exercise and all going forward, the random number generator is pre-seeded for you (with &lt;code&gt;np.random.seed(42)&lt;/code&gt;) to save you typing that each time.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Sampling out of the Binomial distribution&quot;,&quot;^1B&quot;,&quot;# Take 10,000 samples out of the binomial distribution: n_defaults\\n\\n\\n# Compute CDF: x, y\\n\\n\\n# Plot the CDF with axis labels\\n\\n\\n\\n\\n# Show the plot\\n\\n&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Draw samples out of the Binomial distribution using &lt;code&gt;np.random.binomial()&lt;/code&gt;. You should use parameters &lt;code&gt;n = 100&lt;/code&gt; and &lt;code&gt;p = 0.05&lt;/code&gt;, and set the &lt;code&gt;size&lt;/code&gt; keyword argument to &lt;code&gt;10000&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Compute the CDF using your previously-written &lt;code&gt;ecdf()&lt;/code&gt; function.&lt;/li&gt;\\n&lt;li&gt;Plot the CDF with axis labels. The x-axis here is the number of defaults out of 100 loans, while the y-axis is the CDF.&lt;/li&gt;\\n&lt;li&gt;Show the plot.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,10,&quot;sct&quot;,&quot;Ex().check_correct(\\n  check_object(\\&quot;n_defaults\\&quot;).has_equal_value(),\\n  check_function(\\&quot;numpy.random.binomial\\&quot;).multi(\\n    check_args(&#39;n&#39;).has_equal_value(),\\n    check_args(&#39;p&#39;).has_equal_value(),\\n    check_args(&#39;size&#39;).has_equal_value()\\n  )\\n)\\n\\nEx().check_correct(\\n  multi(\\n    check_object(&#39;x&#39;).has_equal_value(),\\n    check_object(&#39;y&#39;).has_equal_value()\\n  ),\\n  check_function(&#39;ecdf&#39;).check_args(0).has_equal_value()\\n)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.plot\\&quot;, index = 0).multi(\\n  check_args([&#39;args&#39;, 0]).has_equal_value(),\\n  check_args([&#39;args&#39;, 1]).has_equal_value(),\\n  check_args([&#39;kwargs&#39;, &#39;marker&#39;]).has_equal_value(),\\n  check_args([&#39;kwargs&#39;, &#39;linestyle&#39;]).has_equal_value()\\n)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.xlabel\\&quot;).check_args(0)\\nEx().check_function(\\&quot;matplotlib.pyplot.ylabel\\&quot;).check_args(0)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\nsuccess_msg(\\&quot;Great work! If you know the story, using built-in algorithms to directly sample out of the distribution is *much* faster.\\&quot;)&quot;,&quot;^1D&quot;,&quot;import numpy as np\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()\\n\\ndef ecdf(data):\\n    return np.sort(data), np.arange(1, len(data)+1) / len(data)\\n\\nnp.random.seed(42)&quot;,&quot;^1E&quot;,&quot;# Take 10,000 samples out of the binomial distribution: n_defaults\\nn_defaults = np.random.binomial(n=100, p=0.05, size=10000)\\n\\n# Compute CDF: x, y\\nx, y = ecdf(n_defaults)\\n\\n# Plot the CDF with axis labels\\n_ = plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;)\\n_ = plt.xlabel(&#39;number of defaults out of 100 loans&#39;)\\n_ = plt.ylabel(&#39;CDF&#39;)\\n\\n# Show the plot\\nplt.show()\\n&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;You can generate samples out of a Binomial distribution using &lt;code&gt;np.random.binomial()&lt;/code&gt;. Refer to the first instruction to see what parameters you need to use, and assign the samples to &lt;code&gt;n_defaults&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Like in the earlier exercise, compute &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; by unpacking the output of &lt;code&gt;ecdf(n_defaults)&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;plt.plot()&lt;/code&gt; with the appropriate arguments, including &lt;code&gt;marker = &#39;.&#39;&lt;/code&gt; and &lt;code&gt;linestyle = &#39;none&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Don&#39;t forget to label your axes with &lt;code&gt;plt.xlabel()&lt;/code&gt; and &lt;code&gt;plt.ylabel()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Finally, as always, use &lt;code&gt;plt.show()&lt;/code&gt; to display the plot.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,0.29768910966744944,&quot;^1Q&quot;,39988],[&quot;^ &quot;,&quot;id&quot;,46156,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;As mentioned in the video, plotting a nice looking PMF requires a bit of matplotlib trickery that we will not go into here. Instead, we will plot the PMF of the Binomial distribution as a histogram with skills you have already learned. The trick is setting up the edges of the bins to pass to &lt;code&gt;plt.hist()&lt;/code&gt; via the &lt;code&gt;bins&lt;/code&gt; keyword argument. We want the bins centered on the integers. So, the edges of the bins should be &lt;code&gt;-0.5, 0.5, 1.5, 2.5, ...&lt;/code&gt; up to &lt;code&gt;max(n_defaults) + 1.5&lt;/code&gt;. You can generate an array like this using &lt;code&gt;np.arange()&lt;/code&gt; and then subtracting &lt;code&gt;0.5&lt;/code&gt; from the array.&lt;/p&gt;\\n&lt;p&gt;You have already sampled out of the Binomial distribution during your exercises on loan defaults, and the resulting samples are in the NumPy array &lt;code&gt;n_defaults&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Plotting the Binomial PMF&quot;,&quot;^1B&quot;,&quot;# Compute bin edges: bins\\nbins = np.arange(____, ____ + ____) - 0.5\\n\\n# Generate histogram\\n\\n\\n# Label axes\\n\\n\\n\\n# Show the plot\\n&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Using &lt;code&gt;np.arange()&lt;/code&gt;, compute the bin edges such that the bins are centered on the integers. Store the resulting array in the variable &lt;code&gt;bins&lt;/code&gt;. &lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;plt.hist()&lt;/code&gt; to plot the histogram of &lt;code&gt;n_defaults&lt;/code&gt; with the &lt;code&gt;normed=True&lt;/code&gt; and &lt;code&gt;bins=bins&lt;/code&gt; keyword arguments.&lt;/li&gt;\\n&lt;li&gt;Show the plot.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,11,&quot;sct&quot;,&quot;Ex().check_correct(\\n  check_object(\\&quot;bins\\&quot;).has_equal_value(),\\n  check_function(\\&quot;numpy.arange\\&quot;, signature = False).multi(\\n    check_args(0).has_equal_value(),\\n    check_args(1).multi(\\n      check_function(&#39;max&#39;, signature = False).check_args(0).has_equal_value(),\\n      has_equal_value()\\n    )\\n  )\\n)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.hist\\&quot;).multi(\\n  check_args(0).has_equal_value(),\\n  check_args(&#39;normed&#39;).has_equal_value(),\\n  check_args(&#39;bins&#39;).has_equal_value()\\n)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.xlabel\\&quot;).check_args(0)\\nEx().check_function(\\&quot;matplotlib.pyplot.ylabel\\&quot;).check_args(0)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\n\\nsuccess_msg(\\&quot;Great work!\\&quot;)&quot;,&quot;^1D&quot;,&quot;import numpy as np\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()\\n\\ndef ecdf(data):\\n    return np.sort(data), np.arange(1, len(data)+1) / len(data)\\n\\nnp.random.seed(42)\\nn_defaults = np.random.binomial(100, 0.05, size=10000)&quot;,&quot;^1E&quot;,&quot;# Compute bin edges: bins\\nbins = np.arange(0, max(n_defaults) + 1.5) - 0.5\\n\\n# Generate histogram\\n_ = plt.hist(n_defaults, normed=True, bins=bins)\\n\\n# Label axes\\n_ = plt.xlabel(&#39;number of defaults out of 100 loans&#39;)\\n_ = plt.ylabel(&#39;PMF&#39;)\\n\\n# Show the plot\\nplt.show()&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;The bin edges can be computed using &lt;code&gt;np.arange()&lt;/code&gt;, going from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;max(n_defaults) + 1.5&lt;/code&gt;. Be sure to subtract &lt;code&gt;0.5&lt;/code&gt; from this array and store it in the variable &lt;code&gt;bins&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;You can generate the histogram with &lt;code&gt;plt.hist()&lt;/code&gt;, including the arguments &lt;code&gt;normed=True&lt;/code&gt; and &lt;code&gt;bins=bins&lt;/code&gt; alongside &lt;code&gt;n_defaults&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Label your axes with &lt;code&gt;plt.xlabel()&lt;/code&gt; and &lt;code&gt;plt.ylabel()&lt;/code&gt;. Note that we are now plotting the &lt;code&gt;PMF&lt;/code&gt; on the Y axis.&lt;/li&gt;\\n&lt;li&gt;Show your plot with &lt;code&gt;plt.show()&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,0.8646902533553504,&quot;^1Q&quot;,46156],[&quot;^ &quot;,&quot;id&quot;,40352,&quot;^&gt;&quot;,&quot;VideoExercise&quot;,&quot;^1A&quot;,null,&quot;^1&quot;,&quot;Poisson processes and the Poisson distribution&quot;,&quot;^1B&quot;,&quot;&quot;,&quot;^1C&quot;,null,&quot;^18&quot;,12,&quot;sct&quot;,&quot;&quot;,&quot;^1D&quot;,&quot;&quot;,&quot;^1E&quot;,&quot;&quot;,&quot;^1F&quot;,null,&quot;^1G&quot;,null,&quot;xp&quot;,50,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1K&quot;,null,&quot;^1L&quot;,null,&quot;^1M&quot;,56.25,&quot;^1N&quot;,&quot;course_1549_8218ee1da960679b60d190d24b50d72f&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,0.8837670325428828,&quot;^1Q&quot;,40352],[&quot;^ &quot;,&quot;id&quot;,39990,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;You just heard that the Poisson distribution is a limit of the Binomial distribution for rare events. This makes sense if you think about the stories. Say we do a Bernoulli trial every minute for an hour, each with a success probability of 0.1. We would do 60 trials, and the number of successes is Binomially distributed, and we would expect to get about 6 successes. This is just like the Poisson story we discussed in the video, where we get on average 6 hits on a website per hour. So, the Poisson distribution with arrival rate equal to \\\\(np\\\\) approximates a Binomial distribution for \\\\(n\\\\) Bernoulli trials with probability \\\\(p\\\\) of success (with \\\\(n\\\\) large and \\\\(p\\\\) small). Importantly, the Poisson distribution is often simpler to work with because it has only one parameter instead of two for the Binomial distribution.&lt;/p&gt;\\n&lt;p&gt;Let&#39;s explore these two distributions computationally. You will compute the mean and standard deviation of samples from a Poisson distribution with an arrival rate of 10. Then, you will compute the mean and standard deviation of samples from a Binomial distribution with parameters \\\\(n\\\\) and \\\\(p\\\\) such that \\\\(np = 10\\\\).&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Relationship between Binomial and Poisson distributions&quot;,&quot;^1B&quot;,&quot;# Draw 10,000 samples out of Poisson distribution: samples_poisson\\n\\n\\n# Print the mean and standard deviation\\nprint(&#39;Poisson:     &#39;, np.mean(samples_poisson),\\n                       np.std(samples_poisson))\\n\\n# Specify values of n and p to consider for Binomial: n, p\\n\\n\\n\\n# Draw 10,000 samples for each n,p pair: samples_binomial\\nfor i in range(3):\\n    samples_binomial = ____\\n\\n    # Print results\\n    print(&#39;n =&#39;, n[i], &#39;Binom:&#39;, np.mean(samples_binomial),\\n                                 np.std(samples_binomial))\\n&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Using the &lt;code&gt;np.random.poisson()&lt;/code&gt; function, draw &lt;code&gt;10000&lt;/code&gt; samples from a Poisson distribution with a mean of &lt;code&gt;10&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Make a list of the &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;p&lt;/code&gt; values to consider for the Binomial distribution. Choose &lt;code&gt;n = [20, 100, 1000]&lt;/code&gt; and &lt;code&gt;p = [0.5, 0.1, 0.01]&lt;/code&gt; so that \\\\(np\\\\) is always 10.&lt;/li&gt;\\n&lt;li&gt;Using &lt;code&gt;np.random.binomial()&lt;/code&gt; inside the provided &lt;code&gt;for&lt;/code&gt; loop, draw &lt;code&gt;10000&lt;/code&gt; samples from a Binomial distribution with each &lt;code&gt;n, p&lt;/code&gt; pair and print the mean and standard deviation of the samples. There are 3 &lt;code&gt;n, p&lt;/code&gt; pairs: &lt;code&gt;20, 0.5&lt;/code&gt;, &lt;code&gt;100, 0.1&lt;/code&gt;, and &lt;code&gt;1000, 0.01&lt;/code&gt;. These can be accessed inside the loop as &lt;code&gt;n[i], p[i]&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,13,&quot;sct&quot;,&quot;Ex().check_correct(\\n\\tcheck_object(\\&quot;samples_poisson\\&quot;).has_equal_value(),\\n  \\tcheck_function(\\&quot;numpy.random.poisson\\&quot;).multi(\\n    \\tcheck_args(0).has_equal_value(),\\n      \\tcheck_args(1).has_equal_value()\\n    )\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;n\\&quot;).has_equal_value(),\\n  \\thas_equal_ast(code = \\&quot;n = [20, 100, 1000]\\&quot;, incorrect_msg = \\&quot;Did you set `n` to a list containing 20, 100, and 1000?\\&quot;)\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;p\\&quot;).has_equal_value(),\\n  \\thas_equal_ast(code = \\&quot;p = [0.5, 0.1, 0.01]\\&quot;, incorrect_msg = \\&quot;Did you set `n` to a list containing 0.5, 0.1, and 0.01?\\&quot;)\\n)\\n\\nEx().check_for_loop().multi(\\n    check_iter().has_equal_value(),\\n    check_body().multi(\\n#         set_context(1).has_equal_output(),\\n#         set_context(2).has_equal_output(),\\n      \\tcheck_function(\\&quot;numpy.random.binomial\\&quot;).multi(\\n        \\tcheck_args(0).has_equal_value(),\\n            check_args(1).has_equal_value(),\\n            check_args(2).has_equal_value()\\n        )\\n    )\\n)\\n\\nsuccess_msg(\\&quot;The means are all about the same, which can be shown to be true by doing some pen-and-paper work. The standard deviation of the Binomial distribution gets closer and closer to that of the Poisson distribution as the probability `p` gets lower and lower.\\&quot;)&quot;,&quot;^1D&quot;,&quot;import numpy as np\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()\\n\\ndef ecdf(data):\\n    return np.sort(data), np.arange(1, len(data)+1) / len(data)\\n\\nnp.random.seed(42)&quot;,&quot;^1E&quot;,&quot;# Draw 10,000 samples out of Poisson distribution: samples_poisson\\nsamples_poisson = np.random.poisson(10, size=10000)\\n\\n# Print the mean and standard deviation\\nprint(&#39;Poisson:     &#39;, np.mean(samples_poisson),\\n                       np.std(samples_poisson))\\n\\n# Specify values of n and p to consider for Binomial: n, p\\nn = [20, 100, 1000]\\np = [0.5, 0.1, 0.01]\\n\\n# Draw 10,000 samples for each n,p pair: samples_binomial\\nfor i in range(3):\\n    samples_binomial = np.random.binomial(n[i], p[i], size=10000)\\n\\n    # Print results\\n    print(&#39;n =&#39;, n[i], &#39;Binom:&#39;, np.mean(samples_binomial),\\n                                 np.std(samples_binomial))\\n&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;To draw samples from a Poisson distribution, you can use the &lt;code&gt;np.random.poisson()&lt;/code&gt; function, passing in the appropriate &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;size&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Follow the second instruction exactly to create the lists containing the values of &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;p&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;To draw samples from a Binomial distribution, you can use &lt;code&gt;np.random.binomial()&lt;/code&gt;. Inside the for loop, be sure to index &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;p&lt;/code&gt; as &lt;code&gt;n[i]&lt;/code&gt; and &lt;code&gt;p[i]&lt;/code&gt; when passing them in as arguments to &lt;code&gt;np.random.binomial()&lt;/code&gt;, and don&#39;t forget to pass in the desired number of samples as well.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,0.29822042195192977,&quot;^1Q&quot;,39990],[&quot;^ &quot;,&quot;id&quot;,45885,&quot;^&gt;&quot;,&quot;MultipleChoiceExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;In baseball, a no-hitter is a game in which a pitcher does not allow the other team to get a hit. This is a rare event, and since the beginning of the so-called modern era of baseball (starting in 1901), there have only been 251 of them through the 2015 season in over 200,000 games. The ECDF of the number of no-hitters in a season is shown to the right. Which probability distribution would be appropriate to describe the number of no-hitters we would expect in a given season?&lt;/p&gt;\\n&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: The no-hitter data set was scraped and calculated from the data sets available at &lt;a href=\\&quot;http://www.retrosheet.org\\&quot;&gt;retrosheet.org&lt;/a&gt; (&lt;a href=\\&quot;http://www.retrosheet.org/notice.txt\\&quot;&gt;license&lt;/a&gt;).&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;How many no-hitters in a season?&quot;,&quot;^1B&quot;,&quot;&quot;,&quot;^1C&quot;,null,&quot;^18&quot;,14,&quot;sct&quot;,&quot;msg1 = \\&quot;\\&quot;\\&quot;If all possibilities of of number of no-hitters were equally likely, what would set the maximum? Possibly the total number of games in a season. If this were the case, a huge number of no-hitters would be equally as probable as none. Does this make sense?\\&quot;\\&quot;\\&quot;\\n\\nmsg2 = \\&quot;\\&quot;\\&quot;Yes, it is Binomial, with each game being a Bernoulli trial. But think about how you can determine the parameters, n and p of the distribution. Further, is p small and n large?\\&quot;\\&quot;\\&quot;\\n\\nmsg3 = \\&quot;\\&quot;\\&quot;Yes, you can use the Poisson distribution. But remember: the Poisson distribution is a limit of the Binomial distribution when the probability of success is small and the number of Bernoulli trials is large.\\&quot;\\&quot;\\&quot;\\n\\nmsg4 = \\&quot;\\&quot;\\&quot;Correct! When we have rare events (low p, high n), the Binomial distribution is Poisson. This has a single parameter, the mean number of successes per time interval, in our case the mean number of no-hitters per season.\\&quot;\\&quot;\\&quot;\\n\\nmsg5 = \\&quot;\\&quot;\\&quot;Yes, the Binomial or Poisson distribution correctly describe the number of no-hitters in a given amount of games. However, the Poisson distribution has a single parameter, while the Binomial distribution has two, as discussed in the previous exercise.\\&quot;\\&quot;\\&quot;\\n\\nEx().has_chosen(correct = 4, msgs = [msg1, msg2, msg3, msg4, msg5])&quot;,&quot;^1D&quot;,&quot;import numpy as np\\nimport pandas as pd\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()\\n\\ndef ecdf(data):\\n    return np.sort(data), np.arange(1, len(data)+1) / len(data)\\n\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1549/datasets/mlb_nohitters.csv&#39;)\\ndf[&#39;year&#39;] = df[&#39;date&#39;].apply(lambda x: int(x/10000))\\nn_nohitters = df.groupby(&#39;year&#39;).count()[&#39;date&#39;]\\nx, y = ecdf(n_nohitters)\\n_ = plt.plot(x, y, &#39;.&#39;)\\n_ = plt.xlabel(&#39;number of no hitters in a season&#39;)\\n_ = plt.ylabel(&#39;ECDF&#39;)\\nplt.show()&quot;,&quot;^1E&quot;,&quot;&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;This can be described using a Binomial distribution, with each game being a Bernoulli trial. Note that here, the probability of success is very small as no-hitters are especially rare events. Also recall that in such cases, the Poisson distribution is a limit of the Binomial distribution. Which is easier to model and compute? Think about how many parameters each has. Fewer parameters makes it easier to model.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,50,&quot;^1H&quot;,[&quot;Discrete uniform&quot;,&quot;Binomial&quot;,&quot;Poisson&quot;,&quot;Both Binomial and Poisson, though Poisson is easier to model and compute.&quot;,&quot;Both Binomial and Poisson, though Binomial is easier to model and compute.&quot;],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,0.8891082362931961,&quot;^1Q&quot;,45885],[&quot;^ &quot;,&quot;id&quot;,46243,&quot;^&gt;&quot;,&quot;NormalExercise&quot;,&quot;^1A&quot;,&quot;&lt;p&gt;1990 and 2015 featured the most no-hitters of any season of baseball (there were seven). Given that there are on average 251/115 no-hitters per season, what is the probability of having seven or more in a season?&lt;/p&gt;&quot;,&quot;^1&quot;,&quot;Was 2015 anomalous?&quot;,&quot;^1B&quot;,&quot;# Draw 10,000 samples out of Poisson distribution: n_nohitters\\n\\n\\n# Compute number of samples that are seven or greater: n_large\\nn_large = np.sum(____)\\n\\n# Compute probability of getting seven or more: p_large\\n\\n\\n# Print the result\\nprint(&#39;Probability of seven or more no-hitters:&#39;, p_large)\\n&quot;,&quot;^1C&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Draw &lt;code&gt;10000&lt;/code&gt; samples from a Poisson distribution with a mean of &lt;code&gt;251/115&lt;/code&gt; and assign to &lt;code&gt;n_nohitters&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Determine how many of your samples had a result greater than or equal to &lt;code&gt;7&lt;/code&gt; and assign to &lt;code&gt;n_large&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Compute the probability, &lt;code&gt;p_large&lt;/code&gt;, of having &lt;code&gt;7&lt;/code&gt; or more no-hitters by dividing &lt;code&gt;n_large&lt;/code&gt; by the total number of samples (&lt;code&gt;10000&lt;/code&gt;).&lt;/li&gt;\\n&lt;li&gt;Hit &#39;Submit Answer&#39; to print the probability that you calculated.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^18&quot;,15,&quot;sct&quot;,&quot;Ex().check_correct(\\n  check_object(\\&quot;n_nohitters\\&quot;).has_equal_value(),\\n  check_function(\\&quot;numpy.random.poisson\\&quot;, signature = False).multi(\\n    check_args(0).has_equal_value(incorrect_msg = &#39;Check your call of `np.random.poisson()`. Did you correctly specify the first argument?&#39;),\\n    check_args(&#39;size&#39;).has_equal_value()\\n  )\\n)\\n\\nEx().check_correct(\\n  check_object(\\&quot;n_large\\&quot;).has_equal_value(),\\n  check_function(\\&quot;numpy.sum\\&quot;, signature = False).check_args(0).has_equal_value()\\n  )\\n\\nEx().check_object(&#39;p_large&#39;).has_equal_value()\\n\\nEx().has_printout(0)\\n\\nsuccess_msg(\\&quot;The result is about 0.007. This means that it is not that improbable to see a 7-or-more no-hitter season in a century. We have seen two in a century and a half, so it is not unreasonable.\\&quot;)&quot;,&quot;^1D&quot;,&quot;import numpy as np\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()\\n\\ndef ecdf(data):\\n    return np.sort(data), np.arange(1, len(data)+1) / len(data)\\n\\nnp.random.seed(42)&quot;,&quot;^1E&quot;,&quot;# Draw 10,000 samples out of Poisson distribution: n_nohitters\\nn_nohitters = np.random.poisson(251/115, size=10000)\\n\\n# Compute number of samples that are seven or greater: n_large\\nn_large = np.sum(n_nohitters &gt;= 7)\\n\\n# Compute probability of getting seven or more: p_large\\np_large = n_large / 10000\\n\\n# Print the result\\nprint(&#39;Probability of seven or more no-hitters:&#39;, p_large)\\n&quot;,&quot;^1F&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;As you have done in earlier exercises, you can draw samples from a Poisson distribution using the &lt;code&gt;np.random.poisson()&lt;/code&gt; function, passing in the &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;size&lt;/code&gt; as arguments.&lt;/li&gt;\\n&lt;li&gt;Recall how in an earlier exercise, you calculated the number of 100-loan simulations with 10 more defaults using &lt;code&gt;np.sum(n_defaults &amp;gt;= 10)&lt;/code&gt;. With a slight tweak to this code, you can calculate &lt;code&gt;n_large&lt;/code&gt; by substituting in the appropriate number of &lt;code&gt;n_nohitters&lt;/code&gt; that the instruction specifies.&lt;/li&gt;\\n&lt;li&gt;To compute &lt;code&gt;p_large&lt;/code&gt;, you just have to divide &lt;code&gt;n_large&lt;/code&gt; by the total number of samples.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;^1G&quot;,null,&quot;xp&quot;,100,&quot;^1H&quot;,[],&quot;^1I&quot;,[],&quot;^1J&quot;,&quot;&quot;,&quot;^1O&quot;,&quot;python&quot;,&quot;^1P&quot;,0.8000212639780735,&quot;^1Q&quot;,46243]]]],&quot;activeImage&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,&quot;course-1549-master:c3a6e751bcf2c130e8e3ece5a875250f-20200121102629113&quot;]],&quot;sharedImage&quot;,[&quot;^0&quot;,[&quot;status&quot;,&quot;NOT_FETCHED&quot;,&quot;data&quot;,null]]]],&quot;systemStatus&quot;,[&quot;^0&quot;,[&quot;indicator&quot;,&quot;none&quot;,&quot;description&quot;,&quot;No status has been fetched from the Status Page.&quot;]],&quot;backendSession&quot;,[&quot;^0&quot;,[&quot;status&quot;,[&quot;^0&quot;,[&quot;code&quot;,&quot;none&quot;,&quot;text&quot;,&quot;&quot;]],&quot;isInitSession&quot;,false,&quot;message&quot;,null]],&quot;settings&quot;,[&quot;^0&quot;,[&quot;uiTheme&quot;,&quot;LIGHT&quot;]],&quot;challengeMode&quot;,[&quot;^0&quot;,[&quot;sampleCodesByExerciseId&quot;,[&quot;^0&quot;,[544,&quot;# Poker and roulette winnings from Monday to Friday:\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\ndays_vector &lt;- c(\\&quot;Monday\\&quot;, \\&quot;Tuesday\\&quot;, \\&quot;Wednesday\\&quot;, \\&quot;Thursday\\&quot;, \\&quot;Friday\\&quot;)\\nnames(poker_vector) &lt;- days_vector\\nnames(roulette_vector) &lt;- days_vector\\n\\n# Select poker results for Monday, Tuesday and Wednesday\\n\\n  \\n# Calculate the average of the elements in poker_start&quot;,545,&quot;# Poker and roulette winnings from Monday to Friday:\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\ndays_vector &lt;- c(\\&quot;Monday\\&quot;, \\&quot;Tuesday\\&quot;, \\&quot;Wednesday\\&quot;, \\&quot;Thursday\\&quot;, \\&quot;Friday\\&quot;)\\nnames(poker_vector) &lt;- days_vector\\nnames(roulette_vector) &lt;- days_vector\\n\\n# Which days did you make money on poker?\\n \\n \\n# Print out selection_vector&quot;,577,&quot;# planets_df is pre-loaded in your workspace\\n\\n# Use order() to create positions\\n\\n\\n# Use positions to sort planets_df&quot;,546,&quot;# Poker and roulette winnings from Monday to Friday:\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\ndays_vector &lt;- c(\\&quot;Monday\\&quot;, \\&quot;Tuesday\\&quot;, \\&quot;Wednesday\\&quot;, \\&quot;Thursday\\&quot;, \\&quot;Friday\\&quot;)\\nnames(poker_vector) &lt;- days_vector\\nnames(roulette_vector) &lt;- days_vector\\n\\n# Which days did you make money on poker?\\nselection_vector &lt;- poker_vector &gt; 0\\n\\n# Select from poker_vector these days&quot;,547,&quot;# Poker and roulette winnings from Monday to Friday:\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\ndays_vector &lt;- c(\\&quot;Monday\\&quot;, \\&quot;Tuesday\\&quot;, \\&quot;Wednesday\\&quot;, \\&quot;Thursday\\&quot;, \\&quot;Friday\\&quot;)\\nnames(poker_vector) &lt;- days_vector\\nnames(roulette_vector) &lt;- days_vector\\n\\n# Which days did you make money on roulette?\\n\\n\\n# Select from roulette_vector these days&quot;,580,&quot;# Vector with numerics from 1 up to 10\\nmy_vector &lt;- 1:10 \\n\\n# Matrix with numerics from 1 up to 9\\nmy_matrix &lt;- matrix(1:9, ncol = 3)\\n\\n# First 10 elements of the built-in data frame mtcars\\nmy_df &lt;- mtcars[1:10,]\\n\\n# Construct list with these different elements:&quot;,549,&quot;# Box office Star Wars (in millions!)\\nnew_hope &lt;- c(460.998, 314.4)\\nempire_strikes &lt;- c(290.475, 247.900)\\nreturn_jedi &lt;- c(309.306, 165.8)\\n\\n# Create box_office\\n\\n\\n# Construct star_wars_matrix&quot;,582,&quot;# The variables mov, act and rev are available\\n\\n# Finish the code to build shining_list&quot;,551,&quot;# Construct star_wars_matrix\\nbox_office &lt;- c(460.998, 314.4, 290.475, 247.900, 309.306, 165.8)\\nstar_wars_matrix &lt;- matrix(box_office, nrow = 3, byrow = TRUE,\\n                           dimnames = list(c(\\&quot;A New Hope\\&quot;, \\&quot;The Empire Strikes Back\\&quot;, \\&quot;Return of the Jedi\\&quot;), \\n                                           c(\\&quot;US\\&quot;, \\&quot;non-US\\&quot;)))\\n\\n# Calculate worldwide box office figures&quot;,552,&quot;# Construct star_wars_matrix\\nbox_office &lt;- c(460.998, 314.4, 290.475, 247.900, 309.306, 165.8)\\nstar_wars_matrix &lt;- matrix(box_office, nrow = 3, byrow = TRUE,\\n                           dimnames = list(c(\\&quot;A New Hope\\&quot;, \\&quot;The Empire Strikes Back\\&quot;, \\&quot;Return of the Jedi\\&quot;), \\n                                           c(\\&quot;US\\&quot;, \\&quot;non-US\\&quot;)))\\n\\n# The worldwide box office figures\\nworldwide_vector &lt;- rowSums(star_wars_matrix)\\n\\n# Bind the new variable worldwide_vector as a column to star_wars_matrix&quot;,553,&quot;# star_wars_matrix and star_wars_matrix2 are available in your workspace\\n\\n# Combine both Star Wars trilogies in one matrix&quot;,554,&quot;# all_wars_matrix is available in your workspace\\n\\n# Select the non-US revenue for all movies\\n\\n  \\n# Average non-US revenue\\n\\n  \\n# Select the non-US revenue for first two movies\\n\\n  \\n# Average non-US revenue for first two movies&quot;,555,&quot;# all_wars_matrix is available in your workspace\\n\\n# Estimate the visitors\\n\\n  \\n# Print the estimate to the console&quot;,556,&quot;# all_wars_matrix and ticket_prices_matrix are available in your workspace\\n\\n# Estimated number of visitors\\n\\n\\n# US visitors\\n\\n\\n# Average number of US visitors&quot;,558,&quot;# Sex vector\\nsex_vector &lt;- c(\\&quot;Male\\&quot;, \\&quot;Female\\&quot;, \\&quot;Female\\&quot;, \\&quot;Male\\&quot;, \\&quot;Male\\&quot;)\\n\\n# Convert sex_vector to a factor\\n\\n\\n# Print out factor_sex_vector&quot;,560,&quot;# Code to build factor_survey_vector\\nsurvey_vector &lt;- c(\\&quot;M\\&quot;, \\&quot;F\\&quot;, \\&quot;F\\&quot;, \\&quot;M\\&quot;, \\&quot;M\\&quot;)\\nfactor_survey_vector &lt;- factor(survey_vector)\\n\\n# Specify the levels of factor_survey_vector&quot;,563,&quot;# Create speed_vector&quot;,4083,&quot;# all_wars_matrix is available in your workspace\\n\\n# Total revenue for US and non-US\\n\\n  \\n# Print out total_revenue_vector&quot;,532,&quot;# Define the variable vegas&quot;,564,&quot;# Create speed_vector\\nspeed_vector &lt;- c(\\&quot;medium\\&quot;, \\&quot;slow\\&quot;, \\&quot;slow\\&quot;, \\&quot;medium\\&quot;, \\&quot;fast\\&quot;)\\n\\n# Convert speed_vector to ordered factor vector\\n\\n\\n# Print factor_speed_vector&quot;,533,&quot;numeric_vector &lt;- c(1, 10, 49)\\ncharacter_vector &lt;- c(\\&quot;a\\&quot;, \\&quot;b\\&quot;, \\&quot;c\\&quot;)\\n\\n# Complete the code for boolean_vector&quot;,565,&quot;# Create factor_speed_vector\\nspeed_vector &lt;- c(\\&quot;medium\\&quot;, \\&quot;slow\\&quot;, \\&quot;slow\\&quot;, \\&quot;medium\\&quot;, \\&quot;fast\\&quot;)\\nfactor_speed_vector &lt;- factor(speed_vector, ordered = TRUE, levels = c(\\&quot;slow\\&quot;, \\&quot;medium\\&quot;, \\&quot;fast\\&quot;))\\n\\n# Factor value for second data analyst\\n\\n\\n# Factor value for fifth data analyst\\n\\n\\n# Is data analyst 2 faster than data analyst 5?&quot;,534,&quot;# Poker winnings from Monday to Friday\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\n\\n# Roulette winnings from Monday to Friday&quot;,536,&quot;# Poker winnings from Monday to Friday\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\n\\n# Roulette winnings from Monday to Friday\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\n\\n# The variable days_vector\\ndays_vector &lt;- c(\\&quot;Monday\\&quot;, \\&quot;Tuesday\\&quot;, \\&quot;Wednesday\\&quot;, \\&quot;Thursday\\&quot;, \\&quot;Friday\\&quot;)\\n \\n# Assign the names of the day to roulette_vector and poker_vector&quot;,537,&quot;A_vector &lt;- c(1, 2, 3)\\nB_vector &lt;- c(4, 5, 6)\\n\\n# Take the sum of A_vector and B_vector\\n  \\n# Print out total_vector&quot;,569,&quot;# Definition of vectors\\nname &lt;- c(\\&quot;Mercury\\&quot;, \\&quot;Venus\\&quot;, \\&quot;Earth\\&quot;, \\&quot;Mars\\&quot;, \\&quot;Jupiter\\&quot;, \\&quot;Saturn\\&quot;, \\&quot;Uranus\\&quot;, \\&quot;Neptune\\&quot;)\\ntype &lt;- c(\\&quot;Terrestrial planet\\&quot;, \\&quot;Terrestrial planet\\&quot;, \\&quot;Terrestrial planet\\&quot;, \\n          \\&quot;Terrestrial planet\\&quot;, \\&quot;Gas giant\\&quot;, \\&quot;Gas giant\\&quot;, \\&quot;Gas giant\\&quot;, \\&quot;Gas giant\\&quot;)\\ndiameter &lt;- c(0.382, 0.949, 1, 0.532, 11.209, 9.449, 4.007, 3.883)\\nrotation &lt;- c(58.64, -243.02, 1, 1.03, 0.41, 0.43, -0.72, 0.67)\\nrings &lt;- c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE)\\n\\n# Create a data frame from the vectors&quot;,538,&quot;# Poker and roulette winnings from Monday to Friday:\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\ndays_vector &lt;- c(\\&quot;Monday\\&quot;, \\&quot;Tuesday\\&quot;, \\&quot;Wednesday\\&quot;, \\&quot;Thursday\\&quot;, \\&quot;Friday\\&quot;)\\nnames(poker_vector) &lt;- days_vector\\nnames(roulette_vector) &lt;- days_vector\\n\\n# Assign to total_daily how much you won/lost on each day&quot;,539,&quot;# Poker and roulette winnings from Monday to Friday:\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\ndays_vector &lt;- c(\\&quot;Monday\\&quot;, \\&quot;Tuesday\\&quot;, \\&quot;Wednesday\\&quot;, \\&quot;Thursday\\&quot;, \\&quot;Friday\\&quot;)\\nnames(poker_vector) &lt;- days_vector\\nnames(roulette_vector) &lt;- days_vector\\n\\n# Total winnings with poker\\ntotal_poker &lt;- sum(poker_vector)\\n\\n# Total winnings with roulette\\n\\n\\n# Total winnings overall\\n\\n\\n# Print out total_week&quot;,540,&quot;# Poker and roulette winnings from Monday to Friday:\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\ndays_vector &lt;- c(\\&quot;Monday\\&quot;, \\&quot;Tuesday\\&quot;, \\&quot;Wednesday\\&quot;, \\&quot;Thursday\\&quot;, \\&quot;Friday\\&quot;)\\nnames(poker_vector) &lt;- days_vector\\nnames(roulette_vector) &lt;- days_vector\\n\\n# Calculate total gains for poker and roulette\\n\\n\\n# Check if you realized higher total gains in poker than in roulette&quot;,541,&quot;# Poker and roulette winnings from Monday to Friday:\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\ndays_vector &lt;- c(\\&quot;Monday\\&quot;, \\&quot;Tuesday\\&quot;, \\&quot;Wednesday\\&quot;, \\&quot;Thursday\\&quot;, \\&quot;Friday\\&quot;)\\nnames(poker_vector) &lt;- days_vector\\nnames(roulette_vector) &lt;- days_vector\\n\\n# Define a new variable based on a selection&quot;,573,&quot;# planets_df is pre-loaded in your workspace\\n\\n# Select the rings variable from planets_df\\n\\n  \\n# Print out rings_vector&quot;,542,&quot;# Poker and roulette winnings from Monday to Friday:\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\ndays_vector &lt;- c(\\&quot;Monday\\&quot;, \\&quot;Tuesday\\&quot;, \\&quot;Wednesday\\&quot;, \\&quot;Thursday\\&quot;, \\&quot;Friday\\&quot;)\\nnames(poker_vector) &lt;- days_vector\\nnames(roulette_vector) &lt;- days_vector\\n\\n# Define a new variable based on a selection&quot;,543,&quot;# Poker and roulette winnings from Monday to Friday:\\npoker_vector &lt;- c(140, -50, 20, -120, 240)\\nroulette_vector &lt;- c(-24, -50, 100, -350, 10)\\ndays_vector &lt;- c(\\&quot;Monday\\&quot;, \\&quot;Tuesday\\&quot;, \\&quot;Wednesday\\&quot;, \\&quot;Thursday\\&quot;, \\&quot;Friday\\&quot;)\\nnames(poker_vector) &lt;- days_vector\\nnames(roulette_vector) &lt;- days_vector\\n\\n# Define a new variable based on a selection&quot;]],&quot;isActive&quot;,false]],&quot;autocomplete&quot;,[&quot;^0&quot;,[]],&quot;mobilePopup&quot;,[&quot;^0&quot;,[]],&quot;user&quot;,[&quot;^0&quot;,[&quot;status&quot;,null,&quot;settings&quot;,[&quot;^0&quot;,[]]]],&quot;chapter&quot;,[&quot;^0&quot;,[&quot;current&quot;,[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,3,&quot;number_of_videos&quot;,4,&quot;slug&quot;,&quot;thinking-probabilistically-discrete-variables&quot;,&quot;last_updated_on&quot;,&quot;21/01/2020&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,15,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1549/slides/ch3_slides.pdf&quot;,&quot;title&quot;,&quot;Thinking probabilistically-- Discrete variables&quot;,&quot;xp&quot;,1150,&quot;id&quot;,3934,&quot;exercises&quot;,[&quot;~#iL&quot;,[[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Probabilistic logic and statistical inference&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=1&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;What is the goal of statistical inference?&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=2&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Why do we use the language of probability?&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=3&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Random number generators and hacker statistics&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=4&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Generating random numbers using the np.random module&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=5&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;The np.random module and Bernoulli trials&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=6&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;How many defaults might we expect?&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=7&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Will the bank fail?&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=8&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Probability distributions and stories: The Binomial distribution&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=9&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Sampling out of the Binomial distribution&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=10&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Plotting the Binomial PMF&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=11&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Poisson processes and the Poisson distribution&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=12&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Relationship between Binomial and Poisson distributions&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,13,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=13&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;MultipleChoiceExercise&quot;,&quot;title&quot;,&quot;How many no-hitters in a season?&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,14,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=14&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Was 2015 anomalous?&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,15,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=15&quot;]]]],&quot;description&quot;,&quot;Statistical inference rests upon probability. Because we can very rarely say anything meaningful with absolute certainty from data, we use probabilistic language to make quantitative statements about data. In this chapter, you will learn how to think probabilistically about discrete quantities: those that can only take certain values, like integers.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]]]],&quot;boot&quot;,[&quot;^0&quot;,[&quot;bootState&quot;,&quot;PRE_BOOTED&quot;,&quot;error&quot;,null]],&quot;location&quot;,[&quot;^0&quot;,[&quot;current&quot;,[&quot;^0&quot;,[&quot;pathname&quot;,&quot;/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables&quot;,&quot;query&quot;,[&quot;^0&quot;,[&quot;ex&quot;,&quot;6&quot;]]]],&quot;canonical&quot;,null]],&quot;authorization&quot;,[&quot;^ &quot;],&quot;course&quot;,[&quot;^0&quot;,[&quot;difficulty_level&quot;,1,&quot;reduced_outline&quot;,null,&quot;marketing_video&quot;,&quot;statistical-thinking-in-python-part-1-marketing-video&quot;,&quot;active_image&quot;,&quot;course-1549-master:c3a6e751bcf2c130e8e3ece5a875250f-20200121102629113&quot;,&quot;author_field&quot;,null,&quot;chapters&quot;,[&quot;^1R&quot;,[[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,1,&quot;number_of_videos&quot;,5,&quot;slug&quot;,&quot;graphical-exploratory-data-analysis&quot;,&quot;last_updated_on&quot;,&quot;21/01/2020&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,15,&quot;free_preview&quot;,true,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1549/slides/ch1_slides.pdf&quot;,&quot;title&quot;,&quot;Graphical exploratory data analysis&quot;,&quot;xp&quot;,1100,&quot;id&quot;,3932,&quot;exercises&quot;,[&quot;^1R&quot;,[[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Introduction to Exploratory Data Analysis&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=1&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Tukey&#39;s comments on EDA&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=2&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Advantages of graphical EDA&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=3&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Plotting a histogram&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=4&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Plotting a histogram of iris data&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=5&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Axis labels!&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=6&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Adjusting the number of bins in a histogram&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=7&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Plot all of your data: Bee swarm plots&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=8&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Bee swarm plot&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=9&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;MultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Interpreting a bee swarm plot&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=10&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Plot all of your data: ECDFs&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=11&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Computing the ECDF&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=12&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Plotting the ECDF&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,13,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=13&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Comparison of ECDFs&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,14,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=14&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Onward toward the whole story!&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,15,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/graphical-exploratory-data-analysis?ex=15&quot;]]]],&quot;description&quot;,&quot;Before diving into sophisticated statistical inference techniques, you should first explore your data by plotting them and computing simple summary statistics. This process, called exploratory data analysis, is a crucial first step in statistical analysis of data.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,2,&quot;number_of_videos&quot;,4,&quot;slug&quot;,&quot;quantitative-exploratory-data-analysis&quot;,&quot;last_updated_on&quot;,&quot;21/01/2020&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,15,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1549/slides/ch2_slides.pdf&quot;,&quot;title&quot;,&quot;Quantitative exploratory data analysis&quot;,&quot;xp&quot;,1200,&quot;id&quot;,3933,&quot;exercises&quot;,[&quot;^1R&quot;,[[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Introduction to summary statistics: The sample mean and median&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=1&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Means and medians&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=2&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Computing means&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=3&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Percentiles, outliers, and box plots&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=4&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Computing percentiles&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=5&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Comparing percentiles to ECDF&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=6&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Box-and-whisker plot&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=7&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Variance and standard deviation&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=8&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Computing the variance&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=9&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;The standard deviation and the variance&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=10&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Covariance and the Pearson correlation coefficient&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=11&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Scatter plots&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=12&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;MultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Variance and covariance by looking&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,13,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=13&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Computing the covariance&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,14,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=14&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Computing the Pearson correlation coefficient&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,15,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/quantitative-exploratory-data-analysis?ex=15&quot;]]]],&quot;description&quot;,&quot;In this chapter, you will compute useful summary statistics, which serve to concisely describe salient features of a dataset with a few numbers.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,3,&quot;number_of_videos&quot;,4,&quot;slug&quot;,&quot;thinking-probabilistically-discrete-variables&quot;,&quot;last_updated_on&quot;,&quot;21/01/2020&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,15,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1549/slides/ch3_slides.pdf&quot;,&quot;title&quot;,&quot;Thinking probabilistically-- Discrete variables&quot;,&quot;xp&quot;,1150,&quot;id&quot;,3934,&quot;exercises&quot;,[&quot;^1R&quot;,[[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Probabilistic logic and statistical inference&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=1&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;What is the goal of statistical inference?&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=2&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Why do we use the language of probability?&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=3&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Random number generators and hacker statistics&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=4&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Generating random numbers using the np.random module&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=5&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;The np.random module and Bernoulli trials&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=6&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;How many defaults might we expect?&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=7&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Will the bank fail?&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=8&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Probability distributions and stories: The Binomial distribution&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=9&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Sampling out of the Binomial distribution&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=10&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Plotting the Binomial PMF&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=11&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Poisson processes and the Poisson distribution&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=12&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Relationship between Binomial and Poisson distributions&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,13,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=13&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;MultipleChoiceExercise&quot;,&quot;title&quot;,&quot;How many no-hitters in a season?&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,14,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=14&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Was 2015 anomalous?&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,15,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=15&quot;]]]],&quot;description&quot;,&quot;Statistical inference rests upon probability. Because we can very rarely say anything meaningful with absolute certainty from data, we use probabilistic language to make quantitative statements about data. In this chapter, you will learn how to think probabilistically about discrete quantities: those that can only take certain values, like integers.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^0&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,4,&quot;number_of_videos&quot;,5,&quot;slug&quot;,&quot;thinking-probabilistically-continuous-variables&quot;,&quot;last_updated_on&quot;,&quot;21/01/2020&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,16,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1549/slides/ch4_slides.pdf&quot;,&quot;title&quot;,&quot;Thinking probabilistically-- Continuous variables&quot;,&quot;xp&quot;,1100,&quot;id&quot;,3935,&quot;exercises&quot;,[&quot;^1R&quot;,[[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Probability density functions&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=1&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;MultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Interpreting PDFs&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=2&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;MultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Interpreting CDFs&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=3&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Introduction to the Normal distribution&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=4&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;The Normal PDF&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=5&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;The Normal CDF&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=6&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;The Normal distribution: Properties and warnings&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=7&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;MultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Gauss and the 10 Deutschmark banknote&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=8&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Are the Belmont Stakes results Normally distributed?&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=9&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;What are the chances of a horse matching or beating Secretariat&#39;s record?&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=10&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;The Exponential distribution&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=11&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Matching a story and a distribution&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=12&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Waiting for the next Secretariat&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,13,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=13&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;If you have a story, you can simulate it!&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,14,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=14&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Distribution of no-hitters and cycles&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,15,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=15&quot;]],[&quot;^0&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Final thoughts&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,16,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-continuous-variables?ex=16&quot;]]]],&quot;description&quot;,&quot;Itâ€™s time to move onto continuous variables, such as those that can take on any fractional value. Many of the principles are the same, but there are some subtleties. At the end of this final chapter, you will be speaking the probabilistic language you need to launch into the inference techniques covered in the sequel to this course.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]]]],&quot;time_needed&quot;,null,&quot;author_image&quot;,&quot;placeholder.png&quot;,&quot;tracks&quot;,[&quot;^1R&quot;,[[&quot;^0&quot;,[&quot;path&quot;,&quot;/tracks/data-science-for-everyone&quot;,&quot;title_with_subtitle&quot;,&quot;Data Science for Everyone&quot;]],[&quot;^0&quot;,[&quot;path&quot;,&quot;/tracks/machine-learning-for-everyone&quot;,&quot;title_with_subtitle&quot;,&quot;Machine Learning for Everyone&quot;]],[&quot;^0&quot;,[&quot;path&quot;,&quot;/tracks/data-scientist-with-python&quot;,&quot;title_with_subtitle&quot;,&quot;Data Scientist  with Python&quot;]],[&quot;^0&quot;,[&quot;path&quot;,&quot;/tracks/statistics-fundamentals-with-python&quot;,&quot;title_with_subtitle&quot;,&quot;Statistics Fundamentals  with Python&quot;]]]],&quot;runtime_config&quot;,null,&quot;lti_only&quot;,false,&quot;image_url&quot;,&quot;https://assets.datacamp.com/production/course_1549/shields/thumb/shield_image_course_1549_20191010-1-13inj9n?1570728356&quot;,&quot;topic_id&quot;,7,&quot;slug&quot;,&quot;statistical-thinking-in-python-part-1&quot;,&quot;last_updated_on&quot;,&quot;15/05/2020&quot;,&quot;paid&quot;,true,&quot;collaborators&quot;,[&quot;^1R&quot;,[[&quot;^0&quot;,[&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/users/avatars/000/804/054/square/headshot.png?1490022370&quot;,&quot;full_name&quot;,&quot;Yashas Roy&quot;]],[&quot;^0&quot;,[&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/users/avatars/000/469/499/square/hugo_bowne-anderson.png?1470132470&quot;,&quot;full_name&quot;,&quot;Hugo Bowne-Anderson&quot;]],[&quot;^0&quot;,[&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/users/avatars/000/596/949/square/VL.png?1477655089&quot;,&quot;full_name&quot;,&quot;Vincent Lan&quot;]]]],&quot;time_needed_in_hours&quot;,3,&quot;technology_id&quot;,2,&quot;university&quot;,null,&quot;archived_at&quot;,null,&quot;state&quot;,&quot;live&quot;,&quot;author_bio&quot;,null,&quot;should_cache&quot;,true,&quot;sharing_links&quot;,[&quot;^0&quot;,[&quot;twitter&quot;,&quot;http://bit.ly/1eWTMJh&quot;,&quot;facebook&quot;,&quot;http://bit.ly/1iS42Do&quot;]],&quot;instructors&quot;,[&quot;^1R&quot;,[[&quot;^0&quot;,[&quot;id&quot;,353914,&quot;marketing_biography&quot;,&quot;Lecturer at the California Institute of Technology&quot;,&quot;biography&quot;,&quot;Justin Bois is a Teaching Professor in the Division of Biology and Biological Engineering at the California Institute of Technology. He teaches nine different classes there, nearly all of which heavily feature Python. He is dedicated to empowering students in the biological sciences with quantitative tools, particularly data analysis skills. Beyond biologists, he is thrilled to develop courses for DataCamp, whose students are an excited bunch of burgeoning data scientists!&quot;,&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/users/avatars/000/353/914/square/Screenshot_2016-10-07_18.54.27.png?1475880916&quot;,&quot;full_name&quot;,&quot;Justin Bois&quot;,&quot;instructor_path&quot;,&quot;/instructors/bois&quot;]]]],&quot;seo_title&quot;,&quot;Statistical Thinking in Python (Part 1)&quot;,&quot;title&quot;,&quot;Statistical Thinking in Python (Part 1)&quot;,&quot;xp&quot;,4550,&quot;image_thumbnail_url&quot;,&quot;https://assets.datacamp.com/production/course_1549/shields/thumb_home/shield_image_course_1549_20191010-1-13inj9n?1570728356&quot;,&quot;short_description&quot;,&quot;Build the foundation you need to think statistically and to speak the language of your data.&quot;,&quot;nb_of_subscriptions&quot;,93913,&quot;seo_description&quot;,&quot;Build the foundation you need to think statistically and to speak the language of your data.&quot;,&quot;type&quot;,&quot;datacamp&quot;,&quot;link&quot;,&quot;https://www.datacamp.com/courses/statistical-thinking-in-python-part-1&quot;,&quot;id&quot;,1549,&quot;datasets&quot;,[&quot;^1R&quot;,[[&quot;^0&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/469/datasets/8fb59b9a99957c3b9b1c82b623aea54d8ccbcd9f/2008_all_states.csv&quot;,&quot;name&quot;,&quot;2008 election results (all states)&quot;]],[&quot;^0&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/469/datasets/e079fddb581197780e1a7b7af2aeeff7242535f0/2008_swing_states.csv&quot;,&quot;name&quot;,&quot;2008 election results (swing states)&quot;]],[&quot;^0&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/469/datasets/7507bfed990379f246b4f166ea8a57ecf31c6c9d/belmont.csv&quot;,&quot;name&quot;,&quot;Belmont Stakes&quot;]],[&quot;^0&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/469/datasets/df23780d215774ff90be0ea93e53f4fb5ebbade8/michelson_speed_of_light.csv&quot;,&quot;name&quot;,&quot;Speed of light&quot;]]]],&quot;description&quot;,&quot;After all of the hard work of acquiring data and getting them into a form you can work with, you ultimately want to make clear, succinct conclusions from them. This crucial last step of a data analysis pipeline hinges on the principles of statistical inference. In this course, you will start building the foundation you need to think statistically, speak the language of your data, and understand what your data is telling you. The foundations of statistical thinking took decades to build, but can be grasped much faster today with the help of computers. With the power of Python-based tools, you will rapidly get up-to-speed and begin thinking statistically by the end of this course.&quot;,&quot;prerequisites&quot;,[&quot;^1R&quot;,[[&quot;^0&quot;,[&quot;path&quot;,&quot;/courses/python-data-science-toolbox-part-2&quot;,&quot;title&quot;,&quot;Python Data Science Toolbox (Part 2)&quot;]]]],&quot;original_image_url&quot;,&quot;https://assets.datacamp.com/production/course_1549/shields/original/shield_image_course_1549_20191010-1-13inj9n?1570728356&quot;,&quot;programming_language&quot;,&quot;python&quot;,&quot;external_slug&quot;,&quot;statistical-thinking-in-python-part-1&quot;]],&quot;exercises&quot;,[&quot;^0&quot;,[&quot;current&quot;,5,&quot;all&quot;,[&quot;^1R&quot;,[[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,39983,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^1R&quot;,[]],&quot;number&quot;,1,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.4574118107856051,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1R&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Probabilistic logic and statistical inference&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,39983,&quot;projector_key&quot;,&quot;course_1549_dae09d85f518dfd3e46d3c0427726dde&quot;,&quot;video_link&quot;,null]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;instructions&quot;,null,&quot;externalId&quot;,39989,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;p&gt;We do statistical inference to draw conclusions about our data: These may be probabilistic, actionable, or general conclusions, but the goal is to be able to make decisions based on these conclusions.&lt;/p&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1R&quot;,[&quot;To draw probabilistic conclusions about what we might expect if we collected the same data again.&quot;,&quot;To draw actionable conclusions from data.&quot;,&quot;To draw more general conclusions from relatively few data or observations.&quot;,&quot;[All of these.]&quot;]],&quot;number&quot;,2,&quot;randomNumber&quot;,0.5578316998821693,&quot;assignment&quot;,&quot;&lt;p&gt;Why do we do statistical inference?&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1R&quot;,[&quot;Yes, this is true, but there is more!&quot;,&quot;Yes, this is true, but there is more!&quot;,&quot;Yes, this is true, but there is more!&quot;,&quot;Correct! Statistical inference involves taking your data to probabilistic conclusions about what you would expect if you took even more data, and you can make decisions based on these conclusions.&quot;]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;What is the goal of statistical inference?&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;id&quot;,39989]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;instructions&quot;,null,&quot;externalId&quot;,45888,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;p&gt;While the English word &#39;probably&#39; is indeed not very precise, in the mathematical context, probabilistic language is actually very precise and provides us a measure of uncertainty.&lt;/p&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1R&quot;,[&quot;Probability provides a measure of uncertainty.&quot;,&quot;[Probabilistic language is not very precise.]&quot;,&quot;Data are almost never exactly the same when acquired again, and probability allows us to say how much we expect them to vary.&quot;]],&quot;number&quot;,3,&quot;randomNumber&quot;,0.3147939331894314,&quot;assignment&quot;,&quot;&lt;p&gt;Which of the following is &lt;em&gt;not&lt;/em&gt; a reason why we use probabilistic language in statistical inference?&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1R&quot;,[&quot;Probability is a measure of uncertainty, and this is crucial because we can quantify what we might expect if the data were acquired again.&quot;,&quot;Correct. Probabilistic language is in fact very precise. It precisely describes uncertainty.&quot;,&quot;Incorrect. We need probability to say how data might vary if acquired again.&quot;]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Why do we use the language of probability?&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;id&quot;,45888]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,40351,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^1R&quot;,[]],&quot;number&quot;,4,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.9728646761118771,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1R&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Random number generators and hacker statistics&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,40351,&quot;projector_key&quot;,&quot;course_1549_05960404f6ed0bdb74b810aeaac8318d&quot;,&quot;video_link&quot;,null]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Seed the random number generator\\n\\n\\n# Initialize random numbers: random_numbers\\nrandom_numbers = ____\\n\\n# Generate random numbers by looping over range(100000)\\nfor i in ____:\\n    random_numbers[i] = ____\\n\\n# Plot a histogram\\n_ = plt.hist(____)\\n\\n# Show the plot\\nplt.show()\\n&quot;,&quot;sct&quot;,&quot;Ex().check_function(&#39;numpy.random.seed&#39;).check_args(0).has_equal_value()\\n\\nEx().check_function(&#39;numpy.empty&#39;, signature = False).check_args(0).has_equal_value()\\n\\nEx().check_correct(\\n  check_object(&#39;random_numbers&#39;).has_equal_value(),\\n  check_for_loop().multi(\\n    check_iter().has_equal_value(),\\n    multi(\\n      check_body().check_function(&#39;numpy.random.random&#39;, signature = False),\\n      check_body().has_equal_ast(incorrect_msg = &#39;Check the first for loop. Did you correctly specify the body?&#39;)\\n    )\\n  )\\n)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.hist\\&quot;).check_args(0).has_equal_value()\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\nsuccess_msg(\\&quot;Good work! The histogram is almost exactly flat across the top, indicating that there is equal chance that a randomly-generated number is in any of the bins of the histogram.\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Seed the random number generator using the seed &lt;code&gt;42&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Initialize an empty array, &lt;code&gt;random_numbers&lt;/code&gt;, of 100,000 entries to store the random numbers. Make sure you use &lt;code&gt;np.empty(100000)&lt;/code&gt; to do this.&lt;/li&gt;\\n&lt;li&gt;Write a &lt;code&gt;for&lt;/code&gt; loop to draw 100,000 random numbers using &lt;code&gt;np.random.random()&lt;/code&gt;, storing them in the &lt;code&gt;random_numbers&lt;/code&gt; array. To do so, loop over &lt;code&gt;range(100000)&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Plot a histogram of &lt;code&gt;random_numbers&lt;/code&gt;. It is not necessary to label the axes in this case because we are just checking the random number generator. Hit &#39;Submit Answer&#39; to show your plot.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,39984,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;You can set a random seed using the function &lt;code&gt;np.random.seed()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;To initialize an empty array of, say, 5 entries, you can use &lt;code&gt;np.empty(5)&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;&lt;code&gt;for i in range(100000)&lt;/code&gt; initializes a for loop that will range over 100,000 values; for each of the 100,000 entries, you can use &lt;code&gt;np.random.random()&lt;/code&gt; to draw a random number. Be sure to store the result in the appropriate index &lt;code&gt;i&lt;/code&gt; of &lt;code&gt;random_numbers&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;You can pass in &lt;code&gt;random_numbers&lt;/code&gt; to &lt;code&gt;plt.hist&lt;/code&gt; to plot a histogram.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1R&quot;,[]],&quot;number&quot;,5,&quot;randomNumber&quot;,0.45736793283436694,&quot;assignment&quot;,&quot;&lt;p&gt;We will be hammering the &lt;code&gt;np.random&lt;/code&gt; module for the rest of this course and its sequel. Actually, you will probably call functions from this module more than any other while wearing your hacker statistician hat. Let&#39;s start by taking its simplest function, &lt;code&gt;np.random.random()&lt;/code&gt; for a test spin. The function returns a random number between zero and one. Call &lt;code&gt;np.random.random()&lt;/code&gt; a few times in the IPython shell. You should see numbers jumping around between zero and one.&lt;/p&gt;\\n&lt;p&gt;In this exercise, we&#39;ll generate lots of random numbers between zero and one, and then plot a histogram of the results. If the numbers are truly random, all bars in the histogram should be of (close to) equal height.&lt;/p&gt;\\n&lt;p&gt;You may have noticed that, in the video, Justin generated 4 random numbers by passing the keyword argument &lt;code&gt;size=4&lt;/code&gt; to &lt;code&gt;np.random.random()&lt;/code&gt;. Such an approach is more efficient than a &lt;code&gt;for&lt;/code&gt; loop: in this exercise, however, you will write a &lt;code&gt;for&lt;/code&gt; loop to experience hacker statistics as the practice of repeating an experiment over and over again.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1R&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Generating random numbers using the np.random module&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()&quot;,&quot;solution&quot;,&quot;# Seed the random number generator\\nnp.random.seed(42)\\n\\n# Initialize random numbers: random_numbers\\nrandom_numbers = np.empty(100000)\\n\\n# Generate random numbers by looping over range(100000)\\nfor i in range(100000):\\n    random_numbers[i] = np.random.random()\\n\\n# Plot a histogram\\n_ = plt.hist(random_numbers)\\n\\n# Show the plot\\nplt.show()\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,39984]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;def perform_bernoulli_trials(n, p):\\n    \\&quot;\\&quot;\\&quot;Perform n Bernoulli trials with success probability p\\n    and return number of successes.\\&quot;\\&quot;\\&quot;\\n    # Initialize number of successes: n_success\\n    n_success = ____\\n\\n    # Perform trials\\n    for i in ____:\\n        # Choose random number between zero and one: random_number\\n\\n\\n        # If less than p, it&#39;s a success so add one to n_success\\n        if ____:\\n            ____\\n\\n    return n_success&quot;,&quot;sct&quot;,&quot;Ex().check_function_def(\\&quot;perform_bernoulli_trials\\&quot;).check_correct(\\n  multi(\\n    check_call(\\&quot;f(10, 0.3)\\&quot;).has_equal_value(pre_code=\\&quot;np.random.seed(42)\\&quot;),\\n    check_call(\\&quot;f(10, 0.9)\\&quot;).has_equal_value(pre_code=\\&quot;np.random.seed(42)\\&quot;)    \\n  ),  \\n  multi(\\n    check_args(0).has_equal_ast(),\\n    check_args(1).has_equal_ast(),\\n    check_body().multi(\\n      has_equal_ast(code = \\&quot;n_success = 0\\&quot;, incorrect_msg = \\&quot;Did you correctly intialize the number of successes?\\&quot;),\\n      check_for_loop().multi(\\n        check_iter().has_equal_ast(),\\n        check_body().multi(\\n          check_function(\\&quot;numpy.random.random\\&quot;, signature = sig_from_obj(\\&quot;numpy.random\\&quot;)),\\n          check_if_else().multi(\\n            check_test().has_equal_ast(incorrect_msg = \\&quot;Make sure to test if `random_number &lt; p`.\\&quot;),\\n            check_body().check_or(\\n              has_equal_ast(incorrect_msg = \\&quot;Did you increment `n_success` by `1`?\\&quot;),\\n              has_equal_ast(code = \\&quot;n_success = n_success + 1\\&quot;, incorrect_msg = \\&quot;Did you increment `n_success` by `1`?\\&quot;),\\n              has_equal_ast(code = \\&quot;n_success = 1 + n_success\\&quot;, incorrect_msg = \\&quot;Did you increment `n_success` by `1`?\\&quot;)\\n            )\\n          )\\n        )\\n      )\\n    )\\n  )\\n)\\n\\nsuccess_msg(\\&quot;Good work!\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Define a function with signature &lt;code&gt;perform_bernoulli_trials(n, p)&lt;/code&gt;.&lt;ul&gt;\\n&lt;li&gt;Initialize to zero a variable &lt;code&gt;n_success&lt;/code&gt; the counter of &lt;code&gt;True&lt;/code&gt;s, which are Bernoulli trial successes.&lt;/li&gt;\\n&lt;li&gt;Write a &lt;code&gt;for&lt;/code&gt; loop where you perform a Bernoulli trial in each iteration  and increment the number of success if the result is &lt;code&gt;True&lt;/code&gt;. Perform &lt;code&gt;n&lt;/code&gt; iterations by looping over &lt;code&gt;range(n)&lt;/code&gt;.&lt;ul&gt;\\n&lt;li&gt;To perform a Bernoulli trial, choose a random number between zero and one using &lt;code&gt;np.random.random()&lt;/code&gt;. If the number you chose is less than &lt;code&gt;p&lt;/code&gt;, increment &lt;code&gt;n_success&lt;/code&gt; (use the &lt;code&gt;+= 1&lt;/code&gt; operator to achieve this).&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;\\n&lt;li&gt;The function returns the number of successes &lt;code&gt;n_success&lt;/code&gt;.&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,47490,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;The number of &#39;successes&#39;, &lt;code&gt;n_success&lt;/code&gt;, we start with is 0, so you need to initialize &lt;code&gt;n_success&lt;/code&gt; before we can perform our trials.&lt;/li&gt;\\n&lt;li&gt;For each of the &lt;code&gt;n&lt;/code&gt; trials, use &lt;code&gt;np.random.random()&lt;/code&gt; to choose a random number between 0 and 1 and store the result in &lt;code&gt;random_number&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;If the &lt;code&gt;random_number&lt;/code&gt; selected is &lt;code&gt;&amp;lt;&lt;/code&gt; than the probability of success &lt;code&gt;p&lt;/code&gt;, the trial is considered a success and &lt;code&gt;n_success&lt;/code&gt; will need to be incremented with &lt;code&gt;+=&lt;/code&gt;. Be sure to pay attention to indentation of your &lt;code&gt;for&lt;/code&gt; loop and &lt;code&gt;if&lt;/code&gt; statement.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1R&quot;,[]],&quot;number&quot;,6,&quot;user&quot;,[&quot;^0&quot;,[&quot;isHintShown&quot;,false,&quot;editorTabs&quot;,[&quot;^0&quot;,[&quot;files/script.py&quot;,[&quot;^0&quot;,[&quot;title&quot;,&quot;script.py&quot;,&quot;isSolution&quot;,false,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,true,&quot;isClosable&quot;,false,&quot;code&quot;,null,&quot;extra&quot;,[&quot;^0&quot;,[]]]]]]]],&quot;outputMarkdownTabs&quot;,[&quot;^0&quot;,[]],&quot;markdown&quot;,[&quot;^0&quot;,[&quot;titles&quot;,[&quot;^1R&quot;,[&quot;Knit PDF&quot;,&quot;Knit HTML&quot;]],&quot;activeTitle&quot;,&quot;Knit HTML&quot;]],&quot;currentXp&quot;,100,&quot;graphicalTabs&quot;,[&quot;^0&quot;,[&quot;plot&quot;,[&quot;^0&quot;,[&quot;extraClass&quot;,&quot;animation--flash&quot;,&quot;title&quot;,&quot;Plots&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;sources&quot;,[&quot;^1R&quot;,[]],&quot;currentIndex&quot;,0]],&quot;dimension&quot;,[&quot;^0&quot;,[&quot;isRealSize&quot;,false,&quot;width&quot;,1,&quot;height&quot;,1]]]],&quot;html&quot;,[&quot;^0&quot;,[&quot;extraClass&quot;,&quot;animation--flash&quot;,&quot;title&quot;,&quot;HTML Viewer&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;sources&quot;,[&quot;^1R&quot;,[]],&quot;currentIndex&quot;,0]]]]]],&quot;feedbackMessages&quot;,[&quot;^1R&quot;,[]],&quot;lastSubmittedCode&quot;,null,&quot;ltiStatus&quot;,[&quot;^0&quot;,[]],&quot;lastSubmitActiveEditorTab&quot;,null,&quot;consoleSqlTabs&quot;,[&quot;^0&quot;,[&quot;query_result&quot;,[&quot;^0&quot;,[&quot;extraClass&quot;,&quot;&quot;,&quot;title&quot;,&quot;query result&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,true,&quot;isNotView&quot;,true,&quot;message&quot;,&quot;No query executed yet...&quot;]]]]]],&quot;consoleTabs&quot;,[&quot;^0&quot;,[&quot;console&quot;,[&quot;^0&quot;,[&quot;title&quot;,&quot;IPython Shell&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,true]],&quot;dimension&quot;,[&quot;^0&quot;,[]]]],&quot;slides&quot;,[&quot;^0&quot;,[&quot;title&quot;,&quot;Slides&quot;,&quot;props&quot;,[&quot;^0&quot;,[&quot;active&quot;,false]]]]]],&quot;inputMarkdownTabs&quot;,[&quot;^0&quot;,[]],&quot;consoleObjectViewTabs&quot;,[&quot;^0&quot;,[]]]],&quot;randomNumber&quot;,0.6138412522323577,&quot;assignment&quot;,&quot;&lt;p&gt;You can think of a Bernoulli trial as a flip of a possibly biased coin. Specifically, each coin flip has a probability \\\\(p\\\\) of landing heads (success) and probability \\\\(1-p\\\\) of landing tails (failure). In this exercise, you will write a function to perform &lt;code&gt;n&lt;/code&gt; Bernoulli trials, &lt;code&gt;perform_bernoulli_trials(n, p)&lt;/code&gt;, which returns the number of successes out of &lt;code&gt;n&lt;/code&gt; Bernoulli trials, each of which has probability &lt;code&gt;p&lt;/code&gt; of success. To perform each Bernoulli trial, use the &lt;code&gt;np.random.random()&lt;/code&gt; function, which returns a random number between zero and one.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1R&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;The np.random module and Bernoulli trials&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\nnp.random.seed(42)&quot;,&quot;solution&quot;,&quot;def perform_bernoulli_trials(n, p):\\n    \\&quot;\\&quot;\\&quot;Perform n Bernoulli trials with success probability p\\n    and return number of successes.\\&quot;\\&quot;\\&quot;\\n    # Initialize number of successes: n_success\\n    n_success = 0\\n\\n    # Perform trials\\n    for i in range(n):\\n        # Choose random number between zero and one: random_number\\n        random_number = np.random.random()\\n\\n        # If less than p, it&#39;s a success  so add one to n_success\\n        if random_number &lt; p:\\n            n_success += 1\\n\\n    return n_success&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,47490]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Seed random number generator\\n\\n\\n# Initialize the number of defaults: n_defaults\\n\\n\\n# Compute the number of defaults\\nfor i in ____:\\n    n_defaults[i] = ____\\n\\n\\n# Plot the histogram with default number of bins; label your axes\\n_ = plt.hist(____, ____)\\n_ = plt.xlabel(&#39;number of defaults out of 100 loans&#39;)\\n_ = plt.ylabel(&#39;probability&#39;)\\n\\n# Show the plot\\n&quot;,&quot;sct&quot;,&quot;Ex().check_function(\\&quot;numpy.random.seed\\&quot;).check_args(0).has_equal_value()\\nEx().check_correct(\\n  check_object(\\&quot;n_defaults\\&quot;).has_equal_value(),\\n  check_function(\\&quot;numpy.empty\\&quot;, signature = sig_from_params(param(\\&quot;shape\\&quot;, param.POSITIONAL_OR_KEYWORD))).check_args(0).has_equal_value()\\n)\\nEx().check_for_loop().multi(\\n\\tcheck_iter().check_function(\\&quot;range\\&quot;, signature = sig_from_params(param(\\&quot;stop\\&quot;, param.POSITIONAL_ONLY))).check_args(0).has_equal_value(),\\n  \\tcheck_body().check_function(\\&quot;perform_bernoulli_trials\\&quot;).multi(\\n    \\tcheck_args(0).has_equal_value(),\\n      \\tcheck_args(1).has_equal_value()\\n    )\\n)\\nEx().check_function(\\&quot;matplotlib.pyplot.hist\\&quot;).multi(\\n\\tcheck_args(0).has_equal_ast(),\\n  \\tcheck_args(\\&quot;normed\\&quot;).has_equal_value()\\n)\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\nsuccess_msg(\\&quot;Nice work! This is actually not an optimal way to plot a histogram when the results are known to be integers. We will revisit this in forthcoming exercises.\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Seed the random number generator to 42.&lt;/li&gt;\\n&lt;li&gt;Initialize &lt;code&gt;n_defaults&lt;/code&gt;, an empty array, using &lt;code&gt;np.empty()&lt;/code&gt;. It should contain 1000 entries, since we are doing 1000 simulations.&lt;/li&gt;\\n&lt;li&gt;Write a &lt;code&gt;for&lt;/code&gt; loop with &lt;code&gt;1000&lt;/code&gt; iterations  to compute the number of defaults per 100 loans using the &lt;code&gt;perform_bernoulli_trials()&lt;/code&gt; function. It accepts two arguments: the number of trials &lt;code&gt;n&lt;/code&gt; - in this case 100 - and the probability of success &lt;code&gt;p&lt;/code&gt; - in this case the probability of a default, which is &lt;code&gt;0.05&lt;/code&gt;. On each iteration of the loop store the result in an entry of &lt;code&gt;n_defaults&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Plot a histogram of &lt;code&gt;n_defaults&lt;/code&gt;. Include the &lt;code&gt;normed=True&lt;/code&gt; keyword argument so that the height of the bars of the histogram indicate the probability.&lt;/li&gt;\\n&lt;li&gt;Show your plot.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,39985,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;As you did earlier, you can set the random seed using &lt;code&gt;np.random.seed(42)&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;An empty array can be initialized using &lt;code&gt;np.empty()&lt;/code&gt;, with the number of entries specified inside the parentheses.&lt;/li&gt;\\n&lt;li&gt;Earlier, you looped over a range of 100,000 values and then n values using &lt;code&gt;for i in range(100000)&lt;/code&gt; and &lt;code&gt;for i in range(n)&lt;/code&gt;, respectively. Here, we need to loop over the number of entries we specified above. While we have 1000 entries, or simulations, we will only be performing 100 bernoulli trials, each of which has a probability of success &lt;code&gt;p = 0.05&lt;/code&gt;; these are the arguments that will need to be passed in to the &lt;code&gt;perform_bernoulli_trials()&lt;/code&gt; function that was defined in the previous exercise. For each trial, the result should be assigned to the appropriate index of &lt;code&gt;n_defaults&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;You can plot a histogram using &lt;code&gt;plt.hist()&lt;/code&gt;, but remember to specify &lt;code&gt;normed=True&lt;/code&gt; in addition to &lt;code&gt;n_defaults&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;plt.show()&lt;/code&gt; to display the plot.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1R&quot;,[]],&quot;number&quot;,7,&quot;randomNumber&quot;,0.0663850191800277,&quot;assignment&quot;,&quot;&lt;p&gt;Let&#39;s say a bank made 100 mortgage loans. It is possible that anywhere between 0 and 100 of the loans will be defaulted upon. You would like to know the probability of getting a given number of defaults, given that the probability of a default is &lt;code&gt;p = 0.05&lt;/code&gt;. To investigate this, you will do a simulation. You will perform 100 Bernoulli trials using the &lt;code&gt;perform_bernoulli_trials()&lt;/code&gt; function you wrote in the previous exercise and record how many defaults we get. Here, a success is a default. (Remember that the word \\&quot;success\\&quot; just means that the Bernoulli trial evaluates to &lt;code&gt;True&lt;/code&gt;, i.e., did the loan recipient default?) You will do this for another 100 Bernoulli trials. And again and again until we have tried it 1000 times. Then, you will plot a histogram describing the probability of the number of defaults.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1R&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;How many defaults might we expect?&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()\\n\\ndef bernoulli_trial(p):\\n    return np.random.random() &lt; p\\n\\ndef perform_bernoulli_trials(n, p):\\n    n_success = 0\\n    for _ in range(n):\\n        n_success += bernoulli_trial(p)\\n    return n_success&quot;,&quot;solution&quot;,&quot;# Seed random number generator\\nnp.random.seed(42)\\n\\n# Initialize the number of defaults: n_defaults\\nn_defaults = np.empty(1000)\\n\\n# Compute the number of defaults\\nfor i in range(1000):\\n    n_defaults[i] = perform_bernoulli_trials(100, 0.05)\\n\\n# Plot the histogram with default number of bins; label your axes\\n_ = plt.hist(n_defaults, normed=True)\\n_ = plt.xlabel(&#39;number of defaults out of 100 loans&#39;)\\n_ = plt.ylabel(&#39;probability&#39;)\\n\\n# Show the plot\\nplt.show()&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,39985]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Compute ECDF: x, y\\n\\n\\n# Plot the ECDF with labeled axes\\n\\n\\n\\n\\n# Show the plot\\n\\n\\n# Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money\\n\\n\\n# Compute and print probability of losing money\\nprint(&#39;Probability of losing money =&#39;, n_lose_money / len(n_defaults))\\n&quot;,&quot;sct&quot;,&quot;Ex().check_correct(\\n  multi(\\n    check_object(&#39;x&#39;).has_equal_value(),\\n    check_object(&#39;y&#39;).has_equal_value()\\n  ),\\n  check_function(&#39;ecdf&#39;).check_args(0).has_equal_value()\\n)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.plot\\&quot;, index = 0).multi(\\n  check_args([&#39;args&#39;, 0]).has_equal_value(),\\n  check_args([&#39;args&#39;, 1]).has_equal_value(),\\n  check_args([&#39;kwargs&#39;, &#39;marker&#39;]).has_equal_value(),\\n  check_args([&#39;kwargs&#39;, &#39;linestyle&#39;]).has_equal_value()\\n)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.xlabel\\&quot;).check_args(0)\\nEx().check_function(\\&quot;matplotlib.pyplot.ylabel\\&quot;).check_args(0)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\nEx().check_correct(\\n  check_object(&#39;n_lose_money&#39;).has_equal_value(),\\n  check_function(&#39;numpy.sum&#39;).check_args(0).has_equal_ast()\\n)\\n\\nEx().has_printout(0)\\n\\nsuccess_msg(\\&quot;As we might expect, we most likely get 5/100 defaults. But we still have about a 2% chance of getting 10 or more defaults out of 100 loans.\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Compute the &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; values for the ECDF of &lt;code&gt;n_defaults&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Plot the ECDF, making sure to label the axes. Remember to include &lt;code&gt;marker = &#39;.&#39;&lt;/code&gt; and &lt;code&gt;linestyle = &#39;none&#39;&lt;/code&gt; in addition to &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; in your call &lt;code&gt;plt.plot()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Show the plot.&lt;/li&gt;\\n&lt;li&gt;Compute the total number of entries in your &lt;code&gt;n_defaults&lt;/code&gt; array that were greater than or equal to 10. To do so, compute a boolean array that tells you whether a given entry of &lt;code&gt;n_defaults&lt;/code&gt; is &lt;code&gt;&amp;gt;= 10&lt;/code&gt;. Then sum all the entries in this array using &lt;code&gt;np.sum()&lt;/code&gt;. For example, &lt;code&gt;np.sum(n_defaults &amp;lt;= 5)&lt;/code&gt; would compute the number of defaults with 5 or &lt;em&gt;fewer&lt;/em&gt; defaults.&lt;/li&gt;\\n&lt;li&gt;The probability that the bank loses money is the fraction of &lt;code&gt;n_defaults&lt;/code&gt; that are greater than or equal to 10. Print this result by hitting &#39;Submit Answer&#39;!&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,39986,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;You can compute the &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; values for the ECDF by unpacking the output of &lt;code&gt;ecdf(n_defaults)&lt;/code&gt; appropriately.&lt;/li&gt;\\n&lt;li&gt;To plot the &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; values you just computed, pass them into the &lt;code&gt;plt.plot()&lt;/code&gt; function. Remember to include &lt;code&gt;marker = &#39;.&#39;&lt;/code&gt; and &lt;code&gt;linestyle = &#39;none&#39;&lt;/code&gt; as arguments inside &lt;code&gt;&#39;plt.plot()&lt;/code&gt;, as well as to give labels using &lt;code&gt;plt.xlabel()&lt;/code&gt; and &lt;code&gt;plt.ylabel()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;plt.show()&lt;/code&gt; to display the ECDF you just plotted.&lt;/li&gt;\\n&lt;li&gt;&lt;code&gt;np.sum()&lt;/code&gt; is a useful function when you need to compute &lt;em&gt;totals&lt;/em&gt; of any kind. You can take advantage of NumPy&#39;s vectorized operations by subsetting &lt;code&gt;n_defaults&lt;/code&gt; appropriately with &lt;code&gt;&amp;gt;=&lt;/code&gt; and passing it into &lt;code&gt;np.sum()&lt;/code&gt;. Be sure to assign the result to &lt;code&gt;n_lose_money&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1R&quot;,[]],&quot;number&quot;,8,&quot;randomNumber&quot;,0.18777558662769356,&quot;assignment&quot;,&quot;&lt;p&gt;Plot the number of defaults you got from the previous exercise, in your namespace as &lt;code&gt;n_defaults&lt;/code&gt;, as a CDF. The &lt;code&gt;ecdf()&lt;/code&gt; function you wrote in the first chapter is available.&lt;/p&gt;\\n&lt;p&gt;If interest rates are such that the bank will lose money if 10 or more of its loans are defaulted upon, what is the probability that the bank will lose money?&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1R&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Will the bank fail?&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\nimport pandas as pd\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()\\n\\n# Load data from last time for speed\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1549/datasets/n_defaults.csv&#39;, header=None)\\nn_defaults = df[0].values\\n\\ndef bernoulli_trial(p):\\n    \\&quot;\\&quot;\\&quot;Perform Bernoulli trial with success probability p.\\&quot;\\&quot;\\&quot;\\n    # Choose random number between zero and one\\n    random_number = np.random.random()\\n\\n    # Check to see if it is less than p and return True, else return False\\n    if random_number &lt; p:\\n        return True\\n    return False\\n\\ndef ecdf(data):\\n    return np.sort(data), np.arange(1, len(data)+1) / len(data)&quot;,&quot;solution&quot;,&quot;# Compute ECDF: x, y\\nx, y = ecdf(n_defaults)\\n\\n# Plot the CDF with labeled axes\\n_ = plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;)\\n_ = plt.xlabel(&#39;number of defaults out of 100&#39;)\\n_ = plt.ylabel(&#39;CDF&#39;)\\n\\n# Show the plot\\nplt.show()\\n\\n# Compute the number of 100-loan simulations with 10 or more defaults: n_lose_money\\nn_lose_money = np.sum(n_defaults &gt;= 10)\\n\\n# Compute and print probability of losing money\\nprint(&#39;Probability of losing money =&#39;, n_lose_money / len(n_defaults))\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,39986]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,39987,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^1R&quot;,[]],&quot;number&quot;,9,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.25456993294015073,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1R&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Probability distributions and stories: The Binomial distribution&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,39987,&quot;projector_key&quot;,&quot;course_1549_9b900d9ba13fadf0da9cd66248cc0840&quot;,&quot;video_link&quot;,null]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Take 10,000 samples out of the binomial distribution: n_defaults\\n\\n\\n# Compute CDF: x, y\\n\\n\\n# Plot the CDF with axis labels\\n\\n\\n\\n\\n# Show the plot\\n\\n&quot;,&quot;sct&quot;,&quot;Ex().check_correct(\\n  check_object(\\&quot;n_defaults\\&quot;).has_equal_value(),\\n  check_function(\\&quot;numpy.random.binomial\\&quot;).multi(\\n    check_args(&#39;n&#39;).has_equal_value(),\\n    check_args(&#39;p&#39;).has_equal_value(),\\n    check_args(&#39;size&#39;).has_equal_value()\\n  )\\n)\\n\\nEx().check_correct(\\n  multi(\\n    check_object(&#39;x&#39;).has_equal_value(),\\n    check_object(&#39;y&#39;).has_equal_value()\\n  ),\\n  check_function(&#39;ecdf&#39;).check_args(0).has_equal_value()\\n)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.plot\\&quot;, index = 0).multi(\\n  check_args([&#39;args&#39;, 0]).has_equal_value(),\\n  check_args([&#39;args&#39;, 1]).has_equal_value(),\\n  check_args([&#39;kwargs&#39;, &#39;marker&#39;]).has_equal_value(),\\n  check_args([&#39;kwargs&#39;, &#39;linestyle&#39;]).has_equal_value()\\n)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.xlabel\\&quot;).check_args(0)\\nEx().check_function(\\&quot;matplotlib.pyplot.ylabel\\&quot;).check_args(0)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\nsuccess_msg(\\&quot;Great work! If you know the story, using built-in algorithms to directly sample out of the distribution is *much* faster.\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Draw samples out of the Binomial distribution using &lt;code&gt;np.random.binomial()&lt;/code&gt;. You should use parameters &lt;code&gt;n = 100&lt;/code&gt; and &lt;code&gt;p = 0.05&lt;/code&gt;, and set the &lt;code&gt;size&lt;/code&gt; keyword argument to &lt;code&gt;10000&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Compute the CDF using your previously-written &lt;code&gt;ecdf()&lt;/code&gt; function.&lt;/li&gt;\\n&lt;li&gt;Plot the CDF with axis labels. The x-axis here is the number of defaults out of 100 loans, while the y-axis is the CDF.&lt;/li&gt;\\n&lt;li&gt;Show the plot.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,39988,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;You can generate samples out of a Binomial distribution using &lt;code&gt;np.random.binomial()&lt;/code&gt;. Refer to the first instruction to see what parameters you need to use, and assign the samples to &lt;code&gt;n_defaults&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Like in the earlier exercise, compute &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; by unpacking the output of &lt;code&gt;ecdf(n_defaults)&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;plt.plot()&lt;/code&gt; with the appropriate arguments, including &lt;code&gt;marker = &#39;.&#39;&lt;/code&gt; and &lt;code&gt;linestyle = &#39;none&#39;&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Don&#39;t forget to label your axes with &lt;code&gt;plt.xlabel()&lt;/code&gt; and &lt;code&gt;plt.ylabel()&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Finally, as always, use &lt;code&gt;plt.show()&lt;/code&gt; to display the plot.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1R&quot;,[]],&quot;number&quot;,10,&quot;randomNumber&quot;,0.29768910966744944,&quot;assignment&quot;,&quot;&lt;p&gt;Compute the probability mass function for the number of defaults we would expect for 100 loans as in the last section, but instead of simulating all of the Bernoulli trials, perform the sampling using &lt;code&gt;np.random.binomial()&lt;/code&gt;. This is identical to the calculation you did in the last set of exercises using your custom-written &lt;code&gt;perform_bernoulli_trials()&lt;/code&gt; function, but far more computationally efficient. Given this extra efficiency, we will take 10,000 samples instead of 1000. After taking the samples, plot the CDF as last time. This CDF that you are plotting is that of the Binomial distribution.&lt;/p&gt;\\n&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: For this exercise and all going forward, the random number generator is pre-seeded for you (with &lt;code&gt;np.random.seed(42)&lt;/code&gt;) to save you typing that each time.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1R&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Sampling out of the Binomial distribution&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()\\n\\ndef ecdf(data):\\n    return np.sort(data), np.arange(1, len(data)+1) / len(data)\\n\\nnp.random.seed(42)&quot;,&quot;solution&quot;,&quot;# Take 10,000 samples out of the binomial distribution: n_defaults\\nn_defaults = np.random.binomial(n=100, p=0.05, size=10000)\\n\\n# Compute CDF: x, y\\nx, y = ecdf(n_defaults)\\n\\n# Plot the CDF with axis labels\\n_ = plt.plot(x, y, marker=&#39;.&#39;, linestyle=&#39;none&#39;)\\n_ = plt.xlabel(&#39;number of defaults out of 100 loans&#39;)\\n_ = plt.ylabel(&#39;CDF&#39;)\\n\\n# Show the plot\\nplt.show()\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,39988]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Compute bin edges: bins\\nbins = np.arange(____, ____ + ____) - 0.5\\n\\n# Generate histogram\\n\\n\\n# Label axes\\n\\n\\n\\n# Show the plot\\n&quot;,&quot;sct&quot;,&quot;Ex().check_correct(\\n  check_object(\\&quot;bins\\&quot;).has_equal_value(),\\n  check_function(\\&quot;numpy.arange\\&quot;, signature = False).multi(\\n    check_args(0).has_equal_value(),\\n    check_args(1).multi(\\n      check_function(&#39;max&#39;, signature = False).check_args(0).has_equal_value(),\\n      has_equal_value()\\n    )\\n  )\\n)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.hist\\&quot;).multi(\\n  check_args(0).has_equal_value(),\\n  check_args(&#39;normed&#39;).has_equal_value(),\\n  check_args(&#39;bins&#39;).has_equal_value()\\n)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.xlabel\\&quot;).check_args(0)\\nEx().check_function(\\&quot;matplotlib.pyplot.ylabel\\&quot;).check_args(0)\\n\\nEx().check_function(\\&quot;matplotlib.pyplot.show\\&quot;)\\n\\n\\nsuccess_msg(\\&quot;Great work!\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Using &lt;code&gt;np.arange()&lt;/code&gt;, compute the bin edges such that the bins are centered on the integers. Store the resulting array in the variable &lt;code&gt;bins&lt;/code&gt;. &lt;/li&gt;\\n&lt;li&gt;Use &lt;code&gt;plt.hist()&lt;/code&gt; to plot the histogram of &lt;code&gt;n_defaults&lt;/code&gt; with the &lt;code&gt;normed=True&lt;/code&gt; and &lt;code&gt;bins=bins&lt;/code&gt; keyword arguments.&lt;/li&gt;\\n&lt;li&gt;Show the plot.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,46156,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;The bin edges can be computed using &lt;code&gt;np.arange()&lt;/code&gt;, going from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;max(n_defaults) + 1.5&lt;/code&gt;. Be sure to subtract &lt;code&gt;0.5&lt;/code&gt; from this array and store it in the variable &lt;code&gt;bins&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;You can generate the histogram with &lt;code&gt;plt.hist()&lt;/code&gt;, including the arguments &lt;code&gt;normed=True&lt;/code&gt; and &lt;code&gt;bins=bins&lt;/code&gt; alongside &lt;code&gt;n_defaults&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Label your axes with &lt;code&gt;plt.xlabel()&lt;/code&gt; and &lt;code&gt;plt.ylabel()&lt;/code&gt;. Note that we are now plotting the &lt;code&gt;PMF&lt;/code&gt; on the Y axis.&lt;/li&gt;\\n&lt;li&gt;Show your plot with &lt;code&gt;plt.show()&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1R&quot;,[]],&quot;number&quot;,11,&quot;randomNumber&quot;,0.8646902533553504,&quot;assignment&quot;,&quot;&lt;p&gt;As mentioned in the video, plotting a nice looking PMF requires a bit of matplotlib trickery that we will not go into here. Instead, we will plot the PMF of the Binomial distribution as a histogram with skills you have already learned. The trick is setting up the edges of the bins to pass to &lt;code&gt;plt.hist()&lt;/code&gt; via the &lt;code&gt;bins&lt;/code&gt; keyword argument. We want the bins centered on the integers. So, the edges of the bins should be &lt;code&gt;-0.5, 0.5, 1.5, 2.5, ...&lt;/code&gt; up to &lt;code&gt;max(n_defaults) + 1.5&lt;/code&gt;. You can generate an array like this using &lt;code&gt;np.arange()&lt;/code&gt; and then subtracting &lt;code&gt;0.5&lt;/code&gt; from the array.&lt;/p&gt;\\n&lt;p&gt;You have already sampled out of the Binomial distribution during your exercises on loan defaults, and the resulting samples are in the NumPy array &lt;code&gt;n_defaults&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1R&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Plotting the Binomial PMF&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()\\n\\ndef ecdf(data):\\n    return np.sort(data), np.arange(1, len(data)+1) / len(data)\\n\\nnp.random.seed(42)\\nn_defaults = np.random.binomial(100, 0.05, size=10000)&quot;,&quot;solution&quot;,&quot;# Compute bin edges: bins\\nbins = np.arange(0, max(n_defaults) + 1.5) - 0.5\\n\\n# Generate histogram\\n_ = plt.hist(n_defaults, normed=True, bins=bins)\\n\\n# Label axes\\n_ = plt.xlabel(&#39;number of defaults out of 100 loans&#39;)\\n_ = plt.ylabel(&#39;PMF&#39;)\\n\\n# Show the plot\\nplt.show()&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,46156]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,40352,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^1R&quot;,[]],&quot;number&quot;,12,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.8837670325428828,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^1R&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Poisson processes and the Poisson distribution&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,40352,&quot;projector_key&quot;,&quot;course_1549_8218ee1da960679b60d190d24b50d72f&quot;,&quot;video_link&quot;,null]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Draw 10,000 samples out of Poisson distribution: samples_poisson\\n\\n\\n# Print the mean and standard deviation\\nprint(&#39;Poisson:     &#39;, np.mean(samples_poisson),\\n                       np.std(samples_poisson))\\n\\n# Specify values of n and p to consider for Binomial: n, p\\n\\n\\n\\n# Draw 10,000 samples for each n,p pair: samples_binomial\\nfor i in range(3):\\n    samples_binomial = ____\\n\\n    # Print results\\n    print(&#39;n =&#39;, n[i], &#39;Binom:&#39;, np.mean(samples_binomial),\\n                                 np.std(samples_binomial))\\n&quot;,&quot;sct&quot;,&quot;Ex().check_correct(\\n\\tcheck_object(\\&quot;samples_poisson\\&quot;).has_equal_value(),\\n  \\tcheck_function(\\&quot;numpy.random.poisson\\&quot;).multi(\\n    \\tcheck_args(0).has_equal_value(),\\n      \\tcheck_args(1).has_equal_value()\\n    )\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;n\\&quot;).has_equal_value(),\\n  \\thas_equal_ast(code = \\&quot;n = [20, 100, 1000]\\&quot;, incorrect_msg = \\&quot;Did you set `n` to a list containing 20, 100, and 1000?\\&quot;)\\n)\\n\\nEx().check_correct(\\n\\tcheck_object(\\&quot;p\\&quot;).has_equal_value(),\\n  \\thas_equal_ast(code = \\&quot;p = [0.5, 0.1, 0.01]\\&quot;, incorrect_msg = \\&quot;Did you set `n` to a list containing 0.5, 0.1, and 0.01?\\&quot;)\\n)\\n\\nEx().check_for_loop().multi(\\n    check_iter().has_equal_value(),\\n    check_body().multi(\\n#         set_context(1).has_equal_output(),\\n#         set_context(2).has_equal_output(),\\n      \\tcheck_function(\\&quot;numpy.random.binomial\\&quot;).multi(\\n        \\tcheck_args(0).has_equal_value(),\\n            check_args(1).has_equal_value(),\\n            check_args(2).has_equal_value()\\n        )\\n    )\\n)\\n\\nsuccess_msg(\\&quot;The means are all about the same, which can be shown to be true by doing some pen-and-paper work. The standard deviation of the Binomial distribution gets closer and closer to that of the Poisson distribution as the probability `p` gets lower and lower.\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Using the &lt;code&gt;np.random.poisson()&lt;/code&gt; function, draw &lt;code&gt;10000&lt;/code&gt; samples from a Poisson distribution with a mean of &lt;code&gt;10&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Make a list of the &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;p&lt;/code&gt; values to consider for the Binomial distribution. Choose &lt;code&gt;n = [20, 100, 1000]&lt;/code&gt; and &lt;code&gt;p = [0.5, 0.1, 0.01]&lt;/code&gt; so that \\\\(np\\\\) is always 10.&lt;/li&gt;\\n&lt;li&gt;Using &lt;code&gt;np.random.binomial()&lt;/code&gt; inside the provided &lt;code&gt;for&lt;/code&gt; loop, draw &lt;code&gt;10000&lt;/code&gt; samples from a Binomial distribution with each &lt;code&gt;n, p&lt;/code&gt; pair and print the mean and standard deviation of the samples. There are 3 &lt;code&gt;n, p&lt;/code&gt; pairs: &lt;code&gt;20, 0.5&lt;/code&gt;, &lt;code&gt;100, 0.1&lt;/code&gt;, and &lt;code&gt;1000, 0.01&lt;/code&gt;. These can be accessed inside the loop as &lt;code&gt;n[i], p[i]&lt;/code&gt;.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,39990,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;To draw samples from a Poisson distribution, you can use the &lt;code&gt;np.random.poisson()&lt;/code&gt; function, passing in the appropriate &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;size&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Follow the second instruction exactly to create the lists containing the values of &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;p&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;To draw samples from a Binomial distribution, you can use &lt;code&gt;np.random.binomial()&lt;/code&gt;. Inside the for loop, be sure to index &lt;code&gt;n&lt;/code&gt; and &lt;code&gt;p&lt;/code&gt; as &lt;code&gt;n[i]&lt;/code&gt; and &lt;code&gt;p[i]&lt;/code&gt; when passing them in as arguments to &lt;code&gt;np.random.binomial()&lt;/code&gt;, and don&#39;t forget to pass in the desired number of samples as well.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1R&quot;,[]],&quot;number&quot;,13,&quot;randomNumber&quot;,0.29822042195192977,&quot;assignment&quot;,&quot;&lt;p&gt;You just heard that the Poisson distribution is a limit of the Binomial distribution for rare events. This makes sense if you think about the stories. Say we do a Bernoulli trial every minute for an hour, each with a success probability of 0.1. We would do 60 trials, and the number of successes is Binomially distributed, and we would expect to get about 6 successes. This is just like the Poisson story we discussed in the video, where we get on average 6 hits on a website per hour. So, the Poisson distribution with arrival rate equal to \\\\(np\\\\) approximates a Binomial distribution for \\\\(n\\\\) Bernoulli trials with probability \\\\(p\\\\) of success (with \\\\(n\\\\) large and \\\\(p\\\\) small). Importantly, the Poisson distribution is often simpler to work with because it has only one parameter instead of two for the Binomial distribution.&lt;/p&gt;\\n&lt;p&gt;Let&#39;s explore these two distributions computationally. You will compute the mean and standard deviation of samples from a Poisson distribution with an arrival rate of 10. Then, you will compute the mean and standard deviation of samples from a Binomial distribution with parameters \\\\(n\\\\) and \\\\(p\\\\) such that \\\\(np = 10\\\\).&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1R&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Relationship between Binomial and Poisson distributions&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()\\n\\ndef ecdf(data):\\n    return np.sort(data), np.arange(1, len(data)+1) / len(data)\\n\\nnp.random.seed(42)&quot;,&quot;solution&quot;,&quot;# Draw 10,000 samples out of Poisson distribution: samples_poisson\\nsamples_poisson = np.random.poisson(10, size=10000)\\n\\n# Print the mean and standard deviation\\nprint(&#39;Poisson:     &#39;, np.mean(samples_poisson),\\n                       np.std(samples_poisson))\\n\\n# Specify values of n and p to consider for Binomial: n, p\\nn = [20, 100, 1000]\\np = [0.5, 0.1, 0.01]\\n\\n# Draw 10,000 samples for each n,p pair: samples_binomial\\nfor i in range(3):\\n    samples_binomial = np.random.binomial(n[i], p[i], size=10000)\\n\\n    # Print results\\n    print(&#39;n =&#39;, n[i], &#39;Binom:&#39;, np.mean(samples_binomial),\\n                                 np.std(samples_binomial))\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,39990]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;msg1 = \\&quot;\\&quot;\\&quot;If all possibilities of of number of no-hitters were equally likely, what would set the maximum? Possibly the total number of games in a season. If this were the case, a huge number of no-hitters would be equally as probable as none. Does this make sense?\\&quot;\\&quot;\\&quot;\\n\\nmsg2 = \\&quot;\\&quot;\\&quot;Yes, it is Binomial, with each game being a Bernoulli trial. But think about how you can determine the parameters, n and p of the distribution. Further, is p small and n large?\\&quot;\\&quot;\\&quot;\\n\\nmsg3 = \\&quot;\\&quot;\\&quot;Yes, you can use the Poisson distribution. But remember: the Poisson distribution is a limit of the Binomial distribution when the probability of success is small and the number of Bernoulli trials is large.\\&quot;\\&quot;\\&quot;\\n\\nmsg4 = \\&quot;\\&quot;\\&quot;Correct! When we have rare events (low p, high n), the Binomial distribution is Poisson. This has a single parameter, the mean number of successes per time interval, in our case the mean number of no-hitters per season.\\&quot;\\&quot;\\&quot;\\n\\nmsg5 = \\&quot;\\&quot;\\&quot;Yes, the Binomial or Poisson distribution correctly describe the number of no-hitters in a given amount of games. However, the Poisson distribution has a single parameter, while the Binomial distribution has two, as discussed in the previous exercise.\\&quot;\\&quot;\\&quot;\\n\\nEx().has_chosen(correct = 4, msgs = [msg1, msg2, msg3, msg4, msg5])&quot;,&quot;instructions&quot;,null,&quot;externalId&quot;,45885,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;This can be described using a Binomial distribution, with each game being a Bernoulli trial. Note that here, the probability of success is very small as no-hitters are especially rare events. Also recall that in such cases, the Poisson distribution is a limit of the Binomial distribution. Which is easier to model and compute? Think about how many parameters each has. Fewer parameters makes it easier to model.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1R&quot;,[&quot;Discrete uniform&quot;,&quot;Binomial&quot;,&quot;Poisson&quot;,&quot;Both Binomial and Poisson, though Poisson is easier to model and compute.&quot;,&quot;Both Binomial and Poisson, though Binomial is easier to model and compute.&quot;]],&quot;number&quot;,14,&quot;randomNumber&quot;,0.8891082362931961,&quot;assignment&quot;,&quot;&lt;p&gt;In baseball, a no-hitter is a game in which a pitcher does not allow the other team to get a hit. This is a rare event, and since the beginning of the so-called modern era of baseball (starting in 1901), there have only been 251 of them through the 2015 season in over 200,000 games. The ECDF of the number of no-hitters in a season is shown to the right. Which probability distribution would be appropriate to describe the number of no-hitters we would expect in a given season?&lt;/p&gt;\\n&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;: The no-hitter data set was scraped and calculated from the data sets available at &lt;a href=\\&quot;http://www.retrosheet.org\\&quot;&gt;retrosheet.org&lt;/a&gt; (&lt;a href=\\&quot;http://www.retrosheet.org/notice.txt\\&quot;&gt;license&lt;/a&gt;).&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1R&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;How many no-hitters in a season?&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\nimport pandas as pd\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()\\n\\ndef ecdf(data):\\n    return np.sort(data), np.arange(1, len(data)+1) / len(data)\\n\\ndf = pd.read_csv(&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1549/datasets/mlb_nohitters.csv&#39;)\\ndf[&#39;year&#39;] = df[&#39;date&#39;].apply(lambda x: int(x/10000))\\nn_nohitters = df.groupby(&#39;year&#39;).count()[&#39;date&#39;]\\nx, y = ecdf(n_nohitters)\\n_ = plt.plot(x, y, &#39;.&#39;)\\n_ = plt.xlabel(&#39;number of no hitters in a season&#39;)\\n_ = plt.ylabel(&#39;ECDF&#39;)\\nplt.show()&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;MultipleChoiceExercise&quot;,&quot;id&quot;,45885]],[&quot;^0&quot;,[&quot;sample_code&quot;,&quot;# Draw 10,000 samples out of Poisson distribution: n_nohitters\\n\\n\\n# Compute number of samples that are seven or greater: n_large\\nn_large = np.sum(____)\\n\\n# Compute probability of getting seven or more: p_large\\n\\n\\n# Print the result\\nprint(&#39;Probability of seven or more no-hitters:&#39;, p_large)\\n&quot;,&quot;sct&quot;,&quot;Ex().check_correct(\\n  check_object(\\&quot;n_nohitters\\&quot;).has_equal_value(),\\n  check_function(\\&quot;numpy.random.poisson\\&quot;, signature = False).multi(\\n    check_args(0).has_equal_value(incorrect_msg = &#39;Check your call of `np.random.poisson()`. Did you correctly specify the first argument?&#39;),\\n    check_args(&#39;size&#39;).has_equal_value()\\n  )\\n)\\n\\nEx().check_correct(\\n  check_object(\\&quot;n_large\\&quot;).has_equal_value(),\\n  check_function(\\&quot;numpy.sum\\&quot;, signature = False).check_args(0).has_equal_value()\\n  )\\n\\nEx().check_object(&#39;p_large&#39;).has_equal_value()\\n\\nEx().has_printout(0)\\n\\nsuccess_msg(\\&quot;The result is about 0.007. This means that it is not that improbable to see a 7-or-more no-hitter season in a century. We have seen two in a century and a half, so it is not unreasonable.\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;Draw &lt;code&gt;10000&lt;/code&gt; samples from a Poisson distribution with a mean of &lt;code&gt;251/115&lt;/code&gt; and assign to &lt;code&gt;n_nohitters&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Determine how many of your samples had a result greater than or equal to &lt;code&gt;7&lt;/code&gt; and assign to &lt;code&gt;n_large&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;Compute the probability, &lt;code&gt;p_large&lt;/code&gt;, of having &lt;code&gt;7&lt;/code&gt; or more no-hitters by dividing &lt;code&gt;n_large&lt;/code&gt; by the total number of samples (&lt;code&gt;10000&lt;/code&gt;).&lt;/li&gt;\\n&lt;li&gt;Hit &#39;Submit Answer&#39; to print the probability that you calculated.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,46243,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\n&lt;li&gt;As you have done in earlier exercises, you can draw samples from a Poisson distribution using the &lt;code&gt;np.random.poisson()&lt;/code&gt; function, passing in the &lt;code&gt;mean&lt;/code&gt; and &lt;code&gt;size&lt;/code&gt; as arguments.&lt;/li&gt;\\n&lt;li&gt;Recall how in an earlier exercise, you calculated the number of 100-loan simulations with 10 more defaults using &lt;code&gt;np.sum(n_defaults &amp;gt;= 10)&lt;/code&gt;. With a slight tweak to this code, you can calculate &lt;code&gt;n_large&lt;/code&gt; by substituting in the appropriate number of &lt;code&gt;n_nohitters&lt;/code&gt; that the instruction specifies.&lt;/li&gt;\\n&lt;li&gt;To compute &lt;code&gt;p_large&lt;/code&gt;, you just have to divide &lt;code&gt;n_large&lt;/code&gt; by the total number of samples.&lt;/li&gt;\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^1R&quot;,[]],&quot;number&quot;,15,&quot;randomNumber&quot;,0.8000212639780735,&quot;assignment&quot;,&quot;&lt;p&gt;1990 and 2015 featured the most no-hitters of any season of baseball (there were seven). Given that there are on average 251/115 no-hitters per season, what is the probability of having seven or more in a season?&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^1R&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Was 2015 anomalous?&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;import numpy as np\\n\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nsns.set()\\n\\ndef ecdf(data):\\n    return np.sort(data), np.arange(1, len(data)+1) / len(data)\\n\\nnp.random.seed(42)&quot;,&quot;solution&quot;,&quot;# Draw 10,000 samples out of Poisson distribution: n_nohitters\\nn_nohitters = np.random.poisson(251/115, size=10000)\\n\\n# Compute number of samples that are seven or greater: n_large\\nn_large = np.sum(n_nohitters &gt;= 7)\\n\\n# Compute probability of getting seven or more: p_large\\np_large = n_large / 10000\\n\\n# Print the result\\nprint(&#39;Probability of seven or more no-hitters:&#39;, p_large)\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,46243]]]],&quot;canRateChapter&quot;,false,&quot;isChapterCompleted&quot;,false]],&quot;onboardingMilestones&quot;,[&quot;^0&quot;,[&quot;isActive&quot;,true,&quot;step&quot;,0]]]]";</script><div id="root"><div class="theme progress-indicator--visible theme--light"><header data-cy="header-container" class="dc-header-campus"><a href="https://www.datacamp.com" data-cy="header-logo" class="dc-header-campus__home"><img alt="datacamp-logo" src="/static/media/logo-full-color.018b48cc.svg" style="max-height:29px"></a><div class="dc-nav-course__container"><nav class="dc-nav-course"><a data-cy="header-previous" class="dc-nav-course__backward" data-tip="true" data-for="nav-tp-prev" href="/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=5"><svg width="12" height="12" aria-label="arrow_2_left icon" class="dc-icon-arrow_2_left dc-nav-course__icon" role="Img" fill="currentColor"><use xlink:href="/static/media/symbols.5cf57581.svg#arrow_2_left"/></svg></a><a href="javascript:void(0)" data-cy="header-outline" class="dc-nav-course__outline" data-tip="true" data-for="nav-tp-outline"><svg width="12" height="12" aria-label="bars icon" class="dc-icon-bars dc-nav-course__icon" role="Img" fill="currentColor"><use xlink:href="/static/media/symbols.5cf57581.svg#bars"/></svg>Course Outline</a><a data-cy="header-next" class="dc-nav-course__forward" data-tip="true" data-for="nav-tp-next" href="/courses/statistical-thinking-in-python-part-1/thinking-probabilistically-discrete-variables?ex=7"><svg width="12" height="12" aria-label="arrow_2_right icon" class="dc-icon-arrow_2_right dc-nav-course__icon" role="Img" fill="currentColor"><use xlink:href="/static/media/symbols.5cf57581.svg#arrow_2_right"/></svg></a></nav></div><nav class="dc-u-fx dc-u-fx-aic dc-u-fx-jcfe dc-u-w-96"><div data-cy="header-session" class="app-status dc-u-fx dc-u-mr-8"><div class="hcSlide-wrapper"></div><svg width="18" height="18" aria-label="circle icon" class="dc-icon-circle dc-u-color-green" role="Img" fill="currentColor"><use xlink:href="/static/media/symbols.5cf57581.svg#circle"/></svg></div><a href="javascript:void(0)" data-cy="header-video" class="ds-icon-action dc-u-fx"><svg tooltip-id="tp-video" tooltip-place="left" width="18" height="18" aria-label="video icon" class="dc-icon-video dc-u-fx" role="Img" fill="currentColor"><use xlink:href="/static/media/symbols.5cf57581.svg#video"/></svg></a><a href="javascript:void(0)" data-cy="header-slides" class="ds-icon-action dc-u-fx"><svg tooltip-id="tp-slides" tooltip-place="left" width="18" height="18" aria-label="pdf icon" class="dc-icon-pdf dc-u-fx" role="Img" fill="currentColor"><use xlink:href="/static/media/symbols.5cf57581.svg#pdf"/></svg></a><a href="javascript:void(0)" data-cy="header-issue" class="ds-icon-action dc-u-fx" data-test-id="header-report-issue-button"><svg tooltip-place="left" tooltip-id="tp-issue" width="18" height="18" aria-label="attention icon" class="dc-icon-attention dc-u-fx" role="Img" fill="currentColor"><use xlink:href="/static/media/symbols.5cf57581.svg#attention"/></svg></a></nav></header><div class="exercise-area"><div data-cy="server-side-loader-placeholder"><aside class="exercise--sidebar" style="width:40%"><div class="exercise--sidebar-content"><div class="listview__outer"><div class="listview__inner"><div class="listview__section"><div><div role="button" class="listview__header"><div class="exercise--sidebar-header"><h5 class="dc-panel__title"><svg width="12" height="12" aria-label="exercise icon" class="dc-icon-exercise dc-u-color-grey-dark dc-u-mr-8" role="Img" fill="currentColor"><use xlink:href="/static/media/symbols.5cf57581.svg#exercise"/></svg>Exercise</h5></div></div></div><div class="listview__content"><div class="exercise--assignment exercise--typography"><h1 class="exercise--title">The np.random module and Bernoulli trials</h1><div class><p>You can think of a Bernoulli trial as a flip of a possibly biased coin. Specifically, each coin flip has a probability \(p\) of landing heads (success) and probability \(1-p\) of landing tails (failure). In this exercise, you will write a function to perform <code>n</code> Bernoulli trials, <code>perform_bernoulli_trials(n, p)</code>, which returns the number of successes out of <code>n</code> Bernoulli trials, each of which has probability <code>p</code> of success. To perform each Bernoulli trial, use the <code>np.random.random()</code> function, which returns a random number between zero and one.</p></div></div></div></div><div class="listview__section" style="min-height:calc(100% - 33px)"><div><div role="button" class="listview__header"><div class="exercise--sidebar-header"><h5 class="dc-panel__title"><svg width="12" height="12" aria-label="checkmark_circle icon" class="dc-icon-checkmark_circle dc-u-color-grey-dark dc-u-mr-8" role="Img" fill="currentColor"><use xlink:href="/static/media/symbols.5cf57581.svg#checkmark_circle"/></svg>Instructions</h5><span class="dc-tag tag--xp">100<!-- --> XP</span></div></div></div><div class="listview__content"><div><div class><div class="exercise--instructions exercise--typography"><div class="exercise--instructions__content"><ul>
<li>Define a function with signature <code>perform_bernoulli_trials(n, p)</code>.<ul>
<li>Initialize to zero a variable <code>n_success</code> the counter of <code>True</code>s, which are Bernoulli trial successes.</li>
<li>Write a <code>for</code> loop where you perform a Bernoulli trial in each iteration  and increment the number of success if the result is <code>True</code>. Perform <code>n</code> iterations by looping over <code>range(n)</code>.<ul>
<li>To perform a Bernoulli trial, choose a random number between zero and one using <code>np.random.random()</code>. If the number you chose is less than <code>p</code>, increment <code>n_success</code> (use the <code>+= 1</code> operator to achieve this).</li></ul></li>
<li>The function returns the number of successes <code>n_success</code>.</li></ul></li>
</ul></div><div class="campus-dc-sct-feedback" tabindex="-1"><div></div><ul class="campus-dc-sct-feedback__tab-list"><a href="javascript:void(0)" class="exercise--show-hint" tooltip-id="tp-hint" tooltip-wrapperstyle="[object Object]" data-cy="exercise-show-hint"><svg width="18" height="18" aria-label="lightbulb icon" class="dc-icon-lightbulb dc-u-mr-4" role="Img" fill="currentColor"><use xlink:href="/static/media/symbols.5cf57581.svg#lightbulb"/></svg><span>Take Hint<!-- --> (-<!-- -->30<!-- --> XP)</span></a></ul></div></div></div></div></div></div></div></div></div></aside><section class="exercise--content" style="width:60%"><div class="exercise-waiting"><div class="global-spinner"><object type="image/svg+xml" data="/static/media/spinner.dd0612cb.svg" aria-label="Loading"></object></div><noscript></noscript></div></section></div><div class="Toastify"></div></div><div class="exercise-footer"><ul data-cy="progress-container" class="dc-progress-indicator"><li class="dc-progress-indicator__item"><a href="javascript:void(0)" class="dc-progress-indicator__bar"><div class="dc-progress-indicator__fill" style="width:0%"></div></a></li><li class="dc-progress-indicator__item"><a href="javascript:void(0)" class="dc-progress-indicator__bar"><div class="dc-progress-indicator__fill" style="width:0%"></div></a></li><li class="dc-progress-indicator__item"><a href="javascript:void(0)" class="dc-progress-indicator__bar"><div class="dc-progress-indicator__fill" style="width:0%"></div></a></li><li class="dc-progress-indicator__item"><a href="javascript:void(0)" class="dc-progress-indicator__bar"><div class="dc-progress-indicator__fill" style="width:0%"></div></a></li></ul></div></div></div><script type="text/x-mathjax-config">MathJax && MathJax.Hub && MathJax.Hub.Config && MathJax.Hub.Config({
        messageStyle: "none"
      });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/static/js/main.2cea37ac.js">
